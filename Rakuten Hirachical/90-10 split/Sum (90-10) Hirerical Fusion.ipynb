{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sum (90-10) Hirerical Fusion.ipynb","provenance":[{"file_id":"1Oyo_451Hk6KzEqVvR2ZftyAEy3BuydbI","timestamp":1618659606049},{"file_id":"1Iwp1sNEfG6ly73_MK_bi3EUEZAC05-lQ","timestamp":1616876662483},{"file_id":"1OCoA3nYHVEcUF5r3Sxaq9PuAqT1U9Lx7","timestamp":1616632198364},{"file_id":"1GEPelG8M4pwx_llUYLlmUO6jfjnUgMPl","timestamp":1616587813712},{"file_id":"1Px7sog6jBh8jhBFgrHAKeqt1maPE48jM","timestamp":1614749164976}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"572320394c06443fb199666934c62dc8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1627f3db2fba4a81be8894debef064d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bf79d63cdbe04652b101ae9c5d9a7b2a","IPY_MODEL_fef488bfaf3040ba943b8285d6eab717"]}},"1627f3db2fba4a81be8894debef064d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf79d63cdbe04652b101ae9c5d9a7b2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2ece545f40f145a5a5138fb6d633ad92","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":55025,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":55025,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ea811bc2f544d24b7b5d917e8bbadf1"}},"fef488bfaf3040ba943b8285d6eab717":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01945d169fff46419f65ba37ecf3bc6e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 55025/55025 [00:01&lt;00:00, 38110.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf418a52fbca4f2f818a5c5b0de049bf"}},"2ece545f40f145a5a5138fb6d633ad92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ea811bc2f544d24b7b5d917e8bbadf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01945d169fff46419f65ba37ecf3bc6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cf418a52fbca4f2f818a5c5b0de049bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95a1b6564d5143e89d620c6c89107de7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0866b5e75224494d975a3444d9bae164","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bc7b205d364c43f4a675afe3ddc7eec1","IPY_MODEL_b4dc3e261b2844668c802f759d895917"]}},"0866b5e75224494d975a3444d9bae164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc7b205d364c43f4a675afe3ddc7eec1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_87479667567347689e0e5a0188f0b6b7","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":597,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":597,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_391cb815527b4172b316afd5b09c7b04"}},"b4dc3e261b2844668c802f759d895917":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5eadea8c77d54e168fbc78d76c705283","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 597/597 [00:00&lt;00:00, 2416.74it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a3e99e18f304573b7acc63fefcdd9d0"}},"87479667567347689e0e5a0188f0b6b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"391cb815527b4172b316afd5b09c7b04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5eadea8c77d54e168fbc78d76c705283":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8a3e99e18f304573b7acc63fefcdd9d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1fCPKaFCgw23"},"source":["# Multimodal Calssification on Rakuten France Dataset\n"]},{"cell_type":"code","metadata":{"id":"UgcVjKpiBRND","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565183791,"user_tz":-120,"elapsed":3402,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"71c68bb4-edb6-4bfd-d32c-6ac1feaadd6e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZouy12lm_Xv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565184434,"user_tz":-120,"elapsed":4011,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"85dbdcac-ac85-436c-b4eb-2323faca0c78"},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["data\t sample_data\t    tr_masks_cam.pt    val_inputs_flau.pt\n","drive\t tr_inputs_cam.pt   tr_masks_flau.pt   val_masks_cam.pt\n","Rakuten  tr_inputs_flau.pt  val_inputs_cam.pt  val_masks_flau.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oG6LhxvoLTKt","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1619565184436,"user_tz":-120,"elapsed":3983,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8b39ef9d-161d-4717-c9a1-ae4e45186af3"},"source":["pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"BhBoZ1r7tBSd","executionInfo":{"status":"ok","timestamp":1619565184437,"user_tz":-120,"elapsed":3956,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir Rakuten"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtpdRXBytE0X","executionInfo":{"status":"ok","timestamp":1619565184438,"user_tz":-120,"elapsed":3940,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#  cd './Rakuten'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Im900y65tWcT","executionInfo":{"status":"ok","timestamp":1619565184440,"user_tz":-120,"elapsed":3927,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir models data "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzp_vuw1MuNS","executionInfo":{"status":"ok","timestamp":1619565184441,"user_tz":-120,"elapsed":3910,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OToS528fzBas","executionInfo":{"status":"ok","timestamp":1619565184442,"user_tz":-120,"elapsed":3894,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cd './Rakuten'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwsYn7UatdVV","executionInfo":{"status":"ok","timestamp":1619565184443,"user_tz":-120,"elapsed":3879,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/90_10_Hirachical_CamemBERT_title.pt' models\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFVZ-GX6SDCp","executionInfo":{"status":"ok","timestamp":1619565184443,"user_tz":-120,"elapsed":3864,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/90_10_Hirachical_CamemBERT_description.pt' models"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrrpaqRTui48","executionInfo":{"status":"ok","timestamp":1619565184444,"user_tz":-120,"elapsed":3848,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_title.pt' models\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSJCJorZR4dU","executionInfo":{"status":"ok","timestamp":1619565184446,"user_tz":-120,"elapsed":3833,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/90_10_Hirachical_FlauBERT_description.pt' models"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEeo6IJHukVz","executionInfo":{"status":"ok","timestamp":1619565184447,"user_tz":-120,"elapsed":3816,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/90_10_RESNET_model.pt' models"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEebtBL70gi6","executionInfo":{"status":"ok","timestamp":1619565184448,"user_tz":-120,"elapsed":3800,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTwB_kSusK2","executionInfo":{"status":"ok","timestamp":1619565184453,"user_tz":-120,"elapsed":3788,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !cp  '/content/drive/My Drive/Rakuten/image.zip' './'"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSgklEwxL-SJ","executionInfo":{"status":"ok","timestamp":1619565184454,"user_tz":-120,"elapsed":3773,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iY2GDPw1MZh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565184917,"user_tz":-120,"elapsed":4213,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8838cc70-35d5-444a-e191-a0e289b5ed7e"},"source":[" ls -la"],"execution_count":17,"outputs":[{"output_type":"stream","text":["total 442916\n","drwxr-xr-x 1 root root      4096 Apr 27 22:19 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root      4096 Apr 27 21:58 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 4 root root      4096 Apr 21 13:38 \u001b[01;34m.config\u001b[0m/\n","-rw------- 1 root root   2731445 Apr 27 22:15 data\n","drwx------ 5 root root      4096 Apr 27 21:59 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 5 root root      4096 Apr 27 22:08 \u001b[01;34mRakuten\u001b[0m/\n","drwxr-xr-x 1 root root      4096 Apr 21 13:39 \u001b[01;34msample_data\u001b[0m/\n","-rw-r--r-- 1 root root 101421816 Apr 27 22:58 tr_inputs_cam.pt\n","-rw-r--r-- 1 root root 101421816 Apr 27 22:58 tr_inputs_flau.pt\n","-rw-r--r-- 1 root root 101421816 Apr 27 22:58 tr_masks_cam.pt\n","-rw-r--r-- 1 root root 101421816 Apr 27 22:58 tr_masks_flau.pt\n","-rw-r--r-- 1 root root  11270904 Apr 27 22:58 val_inputs_cam.pt\n","-rw-r--r-- 1 root root  11270904 Apr 27 22:58 val_inputs_flau.pt\n","-rw-r--r-- 1 root root  11270904 Apr 27 22:58 val_masks_cam.pt\n","-rw-r--r-- 1 root root  11270904 Apr 27 22:58 val_masks_flau.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JhdiQHVFvSj3","executionInfo":{"status":"ok","timestamp":1619565184918,"user_tz":-120,"elapsed":4189,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !unzip  ./image.zip"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAziO2UFwo-G","executionInfo":{"status":"ok","timestamp":1619565184919,"user_tz":-120,"elapsed":4173,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cd '../'"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWx79doRO-pp","executionInfo":{"status":"ok","timestamp":1619565184920,"user_tz":-120,"elapsed":4157,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1kbpdFQwrir","executionInfo":{"status":"ok","timestamp":1619565184921,"user_tz":-120,"elapsed":4138,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#  !cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data "],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXpLoRd1h5xu"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"FIGO5jehh8-2"},"source":["# 1.1 Using Colab GPU for Training\n","\n","Since we‚Äôll be training a large neural network it‚Äôs best to take advantage of the free GPUs and TPUs that Google offers (in this case we‚Äôll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit ü°í Notebook Settings ü°í Hardware accelerator ü°í (GPU)"]},{"cell_type":"code","metadata":{"id":"-hgdtmoPf2al","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565185279,"user_tz":-120,"elapsed":4473,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"e6866520-f640-4d9e-f58a-246abf29d39f"},"source":["import os, time, datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import random\n","import logging\n","tqdm.pandas()\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","#NN Packages\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n","\n","logger = logging.getLogger(__name__)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-DN_gCoh_pG","executionInfo":{"status":"ok","timestamp":1619565185281,"user_tz":-120,"elapsed":4442,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"44c20f3c-0ff3-4fc8-ad97-e511916a3604"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dnUWVTZM8Wc","executionInfo":{"status":"ok","timestamp":1619565185282,"user_tz":-120,"elapsed":4410,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"75f8f2e1-7e45-4e94-9861-08caf0afd539"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Tue Apr 27 23:13:03 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    49W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tibnb-j2mVPo"},"source":["# 1.2. Installing the Hugging Face Library - Image Pretrained Models\n","\n","Install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","We‚Äôve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don‚Äôt provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using.\n","\n","The goal  of the **pretrainedmodels** is to:\n","\n","-  help to reproduce transfer learning setups\n","\n","-  access pretrained ConvNets with a unique interface/API inspired by torchvision."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_umLuXpmYwk","executionInfo":{"status":"ok","timestamp":1619565193101,"user_tz":-120,"elapsed":12194,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f22622ab-ecf8-4bae-81b5-ba87abca4c83"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install pretrainedmodels"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.7/dist-packages (0.7.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.41.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.9.1+cu101)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (2.5.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.8.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LLbB2aTdigvz"},"source":["# 2. Dataset Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WDHFkRyDjxpP"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"wJ0jmI1xxQ7j","executionInfo":{"status":"ok","timestamp":1619565193102,"user_tz":-120,"elapsed":12170,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_data_path = '/content/Rakuten/data'\n","image_data_path = '/content/Rakuten/image' "],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QzA9onxAnhv","executionInfo":{"status":"ok","timestamp":1619565193103,"user_tz":-120,"elapsed":12152,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        #loading the Merged, preprocessed text data and test data\n","        train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # new_train =  train[train['Description'] != \" \"]\n","        # new_train = new_train[new_train['Description'].notna()]\n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  def prepare_test(self, text_col):\n","    \n","        X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","        self.X_test = X_test\n","        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","        self.test_sentences = X_test[text_col].values\n","        return self.test_sentences\n","        "],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8triqwglkYZO"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_BYtGYwkSPk","executionInfo":{"status":"ok","timestamp":1619565194487,"user_tz":-120,"elapsed":13511,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"762dc849-c9f2-4b88-f737-271f5f5331b9"},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'title_desc'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","print(\"Labels: \", len(Preprocess.labels))\n","\n","X_test = Preprocess.prepare_test(text_col)\n","print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Trian:   55025\n","Labels:  55025\n","Test:    597\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":745},"id":"P9jF_ASbMB2A","executionInfo":{"status":"ok","timestamp":1619565194489,"user_tz":-120,"elapsed":13480,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"22e8046c-cbb6-4537-a224-131ec077d38e"},"source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","\n","counter = Counter(Preprocess.labels)\n","for k,v in counter.items():\n","\tper = v / len(Preprocess.labels) * 100\n","\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Class=2, n=1205 (2.190%)\n","Class=4, n=1736 (3.155%)\n","Class=5, n=3792 (6.891%)\n","Class=6, n=2526 (4.591%)\n","Class=7, n=4896 (8.898%)\n","Class=8, n=1578 (2.868%)\n","Class=9, n=4096 (7.444%)\n","Class=3, n=3802 (6.910%)\n","Class=11, n=945 (1.717%)\n","Class=14, n=3872 (7.037%)\n","Class=17, n=4707 (8.554%)\n","Class=12, n=9304 (16.909%)\n","Class=19, n=699 (1.270%)\n","Class=20, n=2140 (3.889%)\n","Class=1, n=320 (0.582%)\n","Class=21, n=2214 (4.024%)\n","Class=22, n=758 (1.378%)\n","Class=18, n=866 (1.574%)\n","Class=23, n=872 (1.585%)\n","Class=24, n=2296 (4.173%)\n","Class=25, n=681 (1.238%)\n","Class=16, n=349 (0.634%)\n","Class=26, n=701 (1.274%)\n","Class=0, n=338 (0.614%)\n","Class=13, n=153 (0.278%)\n","Class=10, n=126 (0.229%)\n","Class=15, n=53 (0.096%)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qguSL6pMHQve","executionInfo":{"status":"ok","timestamp":1619565194490,"user_tz":-120,"elapsed":13449,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9d8f6057-464f-4700-d8a2-c0048da32eb6"},"source":["print(Preprocess.train['Title'].isnull().sum())\n","\n","print(Preprocess.train['Description'].isnull().sum())\n","\n","print(Preprocess.train['Image_id'].isnull().sum())\n","\n","print(Preprocess.train['Product_id'].isnull().sum())\n","\n","print(Preprocess.train['Prdtypecode'].isnull().sum())\n","\n","print(Preprocess.train['labels'].isnull().sum())\n","\n","print(Preprocess.train['product'].isnull().sum()) #top level category\n","\n","print(Preprocess.train['title_desc'].isnull().sum())\n","\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gGj4NDbINdl","executionInfo":{"status":"ok","timestamp":1619565194492,"user_tz":-120,"elapsed":13419,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"fd4520f2-6a45-4fc5-c5e7-a282bec05408"},"source":["\n","Preprocess.train.isnull().values.any()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8797SZsJIVnP","executionInfo":{"status":"ok","timestamp":1619565194495,"user_tz":-120,"elapsed":13393,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b003ae87-ac98-42d1-99ee-a2d4b7858113"},"source":["Preprocess.train.isnull().sum().sum()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"SNdXR21N-nES","executionInfo":{"status":"ok","timestamp":1619565194497,"user_tz":-120,"elapsed":13374,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Preprocess.train.to_csv(\"NewTraining.csv\")\n","# Preprocess.X_test.to_csv(\"NewTest.csv\")"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_arnYcRn43B2","executionInfo":{"status":"ok","timestamp":1619565194499,"user_tz":-120,"elapsed":13352,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"400d7a76-93f9-4947-c0c5-a14cdc52e62e"},"source":["print(type(X_test))\n","print(len(X_test))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","597\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jBCwmgodH5wL","executionInfo":{"status":"ok","timestamp":1619565194500,"user_tz":-120,"elapsed":13316,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"1d1e5ccf-e813-4f08-951b-84f1a23c6751"},"source":["Preprocess.train"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des id√©es de grandeur. Il veut organiser...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>Conqu√©rant Sept Cahier Couverture Polypro 240 ...</td>\n","      <td>CONQUERANT CLASSIQUE Cahier 240 x 320 mm sey√®s...</td>\n","      <td>999581347</td>\n","      <td>344240059</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>18</td>\n","      <td>32</td>\n","      <td>Conqu√©rant Sept Cahier Couverture Polypro 240 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","      <td>Tente pliante V3S5 Pro PVC 500 gr/m¬≤ - 3 x 4m5...</td>\n","      <td>1245644185</td>\n","      <td>3793572222</td>\n","      <td>2582</td>\n","      <td>6</td>\n","      <td>Household</td>\n","      <td>19</td>\n","      <td>293</td>\n","      <td>312</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black</td>\n","      <td>The timeless DSW seat can now be paired with m...</td>\n","      <td>1111840281</td>\n","      <td>1915836983</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>8</td>\n","      <td>94</td>\n","      <td>102</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black The ti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55020</th>\n","      <td>55111</td>\n","      <td>84908</td>\n","      <td>84908</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau R√©glable Ch...</td>\n","      <td>Nom de la marque:oobestAmpoules incluses:OuiCe...</td>\n","      <td>1313620762</td>\n","      <td>4198481300</td>\n","      <td>2060</td>\n","      <td>17</td>\n","      <td>Household</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>55</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau R√©glable Ch...</td>\n","    </tr>\n","    <tr>\n","      <th>55021</th>\n","      <td>55112</td>\n","      <td>84909</td>\n","      <td>84909</td>\n","      <td>espa - kit complet de nage √† contre courant 39...</td>\n","      <td>espa espa - kit complet de nage √† contre coura...</td>\n","      <td>1043841028</td>\n","      <td>853455937</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>17</td>\n","      <td>173</td>\n","      <td>190</td>\n","      <td>espa - kit complet de nage √† contre courant 39...</td>\n","    </tr>\n","    <tr>\n","      <th>55022</th>\n","      <td>55113</td>\n","      <td>84910</td>\n","      <td>84910</td>\n","      <td>V√™tements Pour Animaux Mode Style Chiens Ray√© ...</td>\n","      <td>le t - shirt ray√© mode chiens  petits chiots v...</td>\n","      <td>1158527239</td>\n","      <td>2699568414</td>\n","      <td>2220</td>\n","      <td>22</td>\n","      <td>Household</td>\n","      <td>12</td>\n","      <td>168</td>\n","      <td>180</td>\n","      <td>V√™tements Pour Animaux Mode Style Chiens Ray√© ...</td>\n","    </tr>\n","    <tr>\n","      <th>55023</th>\n","      <td>55114</td>\n","      <td>84912</td>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA d√©co pierre √ò 3.50m x...</td>\n","      <td>Description compl√®te :Kit piscine hors-sol Toi...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA d√©co pierre √ò 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>55024</th>\n","      <td>55115</td>\n","      <td>84914</td>\n","      <td>84914</td>\n","      <td>Table Basse Bois De R√©cup√©ration Massif Base B...</td>\n","      <td>Cette table basse a un design unique et consti...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De R√©cup√©ration Massif Base B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55025 rows √ó 14 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...                                         title_desc\n","0               0  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","1               1  ...  La Guerre Des Tuques Luc a des id&eacute;es de...\n","2               2  ...  Conqu√©rant Sept Cahier Couverture Polypro 240 ...\n","3               3  ...  Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...\n","4               4  ...  Eames Inspired Sxw Chair - Pink - Black The ti...\n","...           ...  ...                                                ...\n","55020       55111  ...  Dimmerable Usb Led Lampe De Bureau R√©glable Ch...\n","55021       55112  ...  espa - kit complet de nage √† contre courant 39...\n","55022       55113  ...  V√™tements Pour Animaux Mode Style Chiens Ray√© ...\n","55023       55114  ...  Kit piscine acier NEVADA d√©co pierre √ò 3.50m x...\n","55024       55115  ...  Table Basse Bois De R√©cup√©ration Massif Base B...\n","\n","[55025 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"zMdM2l5CLi0n","executionInfo":{"status":"ok","timestamp":1619565194501,"user_tz":-120,"elapsed":13283,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"64e69f6e-8483-48e8-8270-2c2b849a1b61"},"source":["Preprocess.X_test"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Court Joyeux No√´l En Peluche Taie Sofa Set Pad...</td>\n","      <td>Joyeux No√´l en peluche court Taie Sofa Set Pad...</td>\n","      <td>1323615566</td>\n","      <td>4231863665</td>\n","      <td>Court Joyeux No√´l En Peluche Taie Sofa Set Pad...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Sauna infrarouge Largo - 170 x 105 x 190 - Pin...</td>\n","      <td>Dimensions : 150x105x190 cm ou 170x105x190 cm ...</td>\n","      <td>1158121321</td>\n","      <td>2695198357</td>\n","      <td>Sauna infrarouge Largo - 170 x 105 x 190 - Pin...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>Carnet De Notes Bloc-Notes Cahierindian Squele...</td>\n","      <td>Taille: En format A5 (144 cm x 21 cm) Caract¬ø¬ø...</td>\n","      <td>1303625028</td>\n","      <td>4159071068</td>\n","      <td>Carnet De Notes Bloc-Notes Cahierindian Squele...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>S4sassy Canard Coton Floral Tablier De Cuisine...</td>\n","      <td>* Ces tabliers color√©s et √©l√©gants floral vous...</td>\n","      <td>1263756226</td>\n","      <td>3921311595</td>\n","      <td>S4sassy Canard Coton Floral Tablier De Cuisine...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>3d Imprimante Filament 1.75mm Abs Multiples Co...</td>\n","      <td>3D Imprimante Filament 1.75mm ABS multiples co...</td>\n","      <td>1302522114</td>\n","      <td>4155420525</td>\n","      <td>3d Imprimante Filament 1.75mm Abs Multiples Co...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>592</th>\n","      <td>929</td>\n","      <td>929</td>\n","      <td>1pcs Vinyle Dining Table Napperons Napperon We...</td>\n","      <td>1pcs vinyle Table √† manger place Tapis nappero...</td>\n","      <td>1237180184</td>\n","      <td>3716580313</td>\n","      <td>1pcs Vinyle Dining Table Napperons Napperon We...</td>\n","    </tr>\n","    <tr>\n","      <th>593</th>\n","      <td>932</td>\n","      <td>932</td>\n","      <td>Vie Du Rail (La) N¬∞2265 Du 18-10-1990</td>\n","      <td>LES ARCHEOLOGUES DU RAIL.</td>\n","      <td>450840330</td>\n","      <td>50292005</td>\n","      <td>Vie Du Rail (La) N¬∞2265 Du 18-10-1990 LES ARCH...</td>\n","    </tr>\n","    <tr>\n","      <th>594</th>\n","      <td>933</td>\n","      <td>933</td>\n","      <td>1pc Led Clignotant Percussion Instruments De M...</td>\n","      <td>1pc LED clignotant Percussion Instruments de m...</td>\n","      <td>1275609312</td>\n","      <td>4006500599</td>\n","      <td>1pc Led Clignotant Percussion Instruments De M...</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>934</td>\n","      <td>934</td>\n","      <td>Micro-billes correctrices pH moins pour piscine</td>\n","      <td>DESCRIPTION :Profitez de votre piscine et du b...</td>\n","      <td>1260304262</td>\n","      <td>3893329037</td>\n","      <td>Micro-billes correctrices pH moins pour piscin...</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>936</td>\n","      <td>936</td>\n","      <td>Train Simulator: West Somerset Railway Route (...</td>\n","      <td>NOTE : N√©cessite le jeu original Train Simula...</td>\n","      <td>1260480575</td>\n","      <td>3893886625</td>\n","      <td>Train Simulator: West Somerset Railway Route (...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>597 rows √ó 7 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0  ...                                         title_desc\n","0             1  ...  Court Joyeux No√´l En Peluche Taie Sofa Set Pad...\n","1             2  ...  Sauna infrarouge Largo - 170 x 105 x 190 - Pin...\n","2             4  ...  Carnet De Notes Bloc-Notes Cahierindian Squele...\n","3             8  ...  S4sassy Canard Coton Floral Tablier De Cuisine...\n","4             9  ...  3d Imprimante Filament 1.75mm Abs Multiples Co...\n","..          ...  ...                                                ...\n","592         929  ...  1pcs Vinyle Dining Table Napperons Napperon We...\n","593         932  ...  Vie Du Rail (La) N¬∞2265 Du 18-10-1990 LES ARCH...\n","594         933  ...  1pc Led Clignotant Percussion Instruments De M...\n","595         934  ...  Micro-billes correctrices pH moins pour piscin...\n","596         936  ...  Train Simulator: West Somerset Railway Route (...\n","\n","[597 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"VAqoRpq9zMS4"},"source":["# View Tokenizer Input "]},{"cell_type":"code","metadata":{"id":"lhxfSqOKlR94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565194977,"user_tz":-120,"elapsed":13726,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"05088264-61a7-4a9d-c8c4-fb10ded91363"},"source":["Preprocess.get_sentences(text_col,True)\n","sentences = Preprocess.sentences\n","\n","labels = Preprocess.labels\n","print(sentences)\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["['Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U.<br> Pour un confort optimal et une pr√©cision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapt√© √† votre main mais aussi tr√®s √©l√©gant.<br> Il est livr√© avec un support qui se fixe sans adh√©sif √† l\\'arri√®re du GamePad<br> <br> Caract√©ristiques:<br> Mod√®le: Speedlink PILOT STYLE Touch Pen<br> Couleur: Bleu<br> Ref. Fabricant: SL-3468-BE<br> Compatibilit√©: GamePad Nintendo Wii U<br> Forme particuli√®rement ergonomique excellente tenue en main<br> Pointe √† rev√™tement longue dur√©e con√ßue pour ne pas ab√Æmer l\\'√©cran tactile<br> En bonus : Support inclu pour GamePad<br> <span class=\"vga_style2\"><b></b><br>'\n"," \"La Guerre Des Tuques Luc a des id&eacute;es de grandeur. Il veut organiser un jeu de guerre de boules de neige et s'arranger pour en &ecirc;tre le vainqueur incontest&eacute;. Mais Sophie s'en m&ecirc;le et chambarde tous ses plans...\"\n"," 'Conqu√©rant Sept Cahier Couverture Polypro 240 X 320 Mm 96 Pages 90g Sey√®s Incolore CONQUERANT CLASSIQUE Cahier 240 x 320 mm sey√®s incolorecouverture en Polypro 96 pages agraf√© papier de 90 g/m2(400006764)'\n"," ...\n"," 'V√™tements Pour Animaux Mode Style Chiens Ray√© T-Shirt Costume Petit Chiot Rouge le t - shirt ray√© mode chiens  petits chiots v√™tements<ul><li>note: veuillez comparer le d√©tail tailles avec toi avant d acheter.!!utiliser les m√™mes</li><li>les v√™tements de comparer avec la taille.</li><li>description:</li><li>100% brand new la qualit√© √©lev√©e</li><li>quantit√©: 1</li><li>mat√©riel: coton</li><li>motif: le</li><li>votre chien est plus √©l√©gante cool</li><li>parfait pour la marche  jogging</li><li>attention: comme diff√©rents ordinateurs afficher les couleurs diff√©remment  la couleur de la poste peut varier l√©g√®rement d images ci - dessus  merci pour votre compr√©hension.</li><li>toutes les dimensions sont mesur√©es √† la main  il y a peut - √™tre 2-3cm d√©viations  merci pour ta compr√©hension</li><li>taille des d√©tails:</li><li>taille: s</li><li>cou: 24 cm / 9 h 45  </li><li>dos: 23cm / 9.06  </li><li>bust: 34cm / 13.39  </li><li>taille: m </li><li>cou: 28 / 11.02  </li><li>dos: isbn / 10 63  </li><li>bust: 40 cm / 15.75</li><li>taille: l</li><li>cou: 34cm / 13.39  </li><li>retour: 31cm / 12.20  </li><li>buste: 44 / 17.32 po</li><li>taille: xl</li><li>cou: 38 cm / 14.96  </li><li>: 37cm / 14.57 po</li><li>bust: 51cm / 20.08  </li><li>taille: xxl</li><li>cou: 42cm / 16.54 po</li><li>: 41cm / 16.14  </li><li>buste: 60 cm / 23.62 po</li><li>forfait comprend:</li><li>le t - shirt 1pcs pet</li><li></li></ul>'\n"," 'Kit piscine acier NEVADA d√©co pierre √ò 3.50m x 0.90m <b>Description compl√®te :</b><br />Kit piscine hors-sol Toi PIEDRA GRIS ronde √ò 3.50m hauteur 0.90m. Parois acier liner 30/100eme uni bleu rev√™tement brevet√© exclusif imitant la pierre √©chelle profil√©s en PVC. Kit piscine complet.<br /><br /><b>Caract√©ristiques d√©taill√©es :</b><br />- Forme : Ronde<br />- Type : Kit piscine acier hors-sol<br />- Dimensions ext√©rieures : √ò3.50 x 0.90m<br />- Surface installation : 3.60m x 3.60m<br />- Hauteur avec margelle : 0.90m<br />- Utilisation : Hors-sol<br />- Capacit√© : 8m3<br />- Kit complet : Oui<br />- Liner : Uni bleu 30/100e avec fixation overlap<br />- Largeur des margelles : Sans<br />- Structure : Parois acier anti-corrosion<br />- Epaisseur des parois : Acier 35/100eme<br />- Jambes de force : Sans jambes de forces apparentes<br />- Rev√™tement ext√©rieur : Parois laqu√©es avec d√©coration pierre &#34;Stone Effect&#34;<br />- Type de filtration : Filtration √† cartouche<br />- D√©bit : 2m¬≥ / heure<br />- Pompe : 46W<br />- Garantie structure : 2 ans<br />- Garantie liner : 2 ans<br />- Garantie filtration : 2 ans<br />- Echelle : Sym√©trique acier 2 marches<br />- Notice de montage : Oui<br />- Livraison : 1 palette<br />- Garantie : 2 ans<br />'\n"," 'Table Basse Bois De R√©cup√©ration Massif Base Blanche 60x60x33cm <p>Cette table basse a un design unique et constituera un ajout intemporel √† votre maison. Son dessus de table en bois massif est id√©al pour ranger vos boissons panier de fruits ou objets d√©coratifs et sa base en acier solide ajoute √† la robustesse de la table d&#39;appoint. La table basse est faite de bois de r√©cup√©ration massif provenant de solives de planchers et de poutres de soutien de vieux b√¢timents en cours de d√©molition et peut √™tre compos√©e de diff√©rents types de bois comme le Sesham (bois de rose) le pin le teck le h√™tre le ch√™ne le c√®dre le bois de manguier l&#39;acacia etc. Cela signifie que le bois de r√©cup√©ration conserve les caract√©ristiques de ces diff√©rents types de bois. Le bois r√©cup√©r√© est d√©j√† vieilli patin√© et s√©ch√© de sorte qu&#39;il ne r√©tr√©cit pas ne se plie pas et n&#39;a pas besoin d&#39;une finition. Chaque √©tape du processus est r√©alis√©e avec le plus grand soin que ce soit le pon√ßage la peinture ou le laquage. Les belles fibres de bois rendent chaque meuble unique et l√©g√®rement diff√©rent du suivant. Les signes d&#39;usure et la structure fibreuse visible donnent √† chaque pi√®ce son histoire et un aspect unique. L&#39;article est d√©j√† assembl√© ; aucun assemblage n&#39;est requis. Remarque importante : les couleurs varient d&#39;un meuble √† l&#39;autre rendant chacune de nos tables basses unique la livraison est al√©atoire.</p> <ul><li>Couleur de base : Blanc</li><li>Mat√©riau : dessus de table en bois massif de r√©cup√©ration &#43; base en acier</li><li>Dimensions : 60 x 33 cm (Diam. x H)</li><li>Produit ponc√© peint et laqu√©</li><li>Aucun assemblage requis</li></ul>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXlPVw5U1pos","executionInfo":{"status":"ok","timestamp":1619565194978,"user_tz":-120,"elapsed":13697,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"1abe66e1-966d-4a4b-f5d8-8790e3c7e042"},"source":["print (type(sentences))\n","print()\n","print(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","\n","Total number of sentences:55025, labels:55025\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xwZDh0jf9FRc"},"source":["# View Test Sentences"]},{"cell_type":"code","metadata":{"id":"Sjmz7l6rnefT","executionInfo":{"status":"ok","timestamp":1619565194980,"user_tz":-120,"elapsed":13678,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# batch_size = 32  \n","\n","test_sentences = Preprocess.test_sentences\n","\n","X_test_phase1  = Preprocess.X_test"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8QV2kcCt9Oz6","executionInfo":{"status":"ok","timestamp":1619565194981,"user_tz":-120,"elapsed":13658,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9929cfe2-df37-48f4-bffc-9b42627d3c04"},"source":["print(len(test_sentences))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["597\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oVemBMlCk4xW"},"source":["**Helper Function**"]},{"cell_type":"code","metadata":{"id":"WV-Xbdjxk3Ii","executionInfo":{"status":"ok","timestamp":1619565194982,"user_tz":-120,"elapsed":13638,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbKzQwxlrZm"},"source":["# 3. Tokenization & Input Formatting\n"," Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"Ud5qFfdmlwsk"},"source":["# 3.1. BERT Tokenizer\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"id":"AbEg-aB2l61a","executionInfo":{"status":"ok","timestamp":1619565196264,"user_tz":-120,"elapsed":14905,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import XLMForSequenceClassification\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","from torch.nn import Dropout,Conv1d, Linear\n","from transformers.modeling_utils import SequenceSummary\n","\n","#from transformers.modeling_roberta import RobertaClassificationHead\n"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l3oZ1Tims3q"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","Add special tokens to the start and end of each sentence.\n","Pad & truncate all sentences to a single constant length.\n","Explicitly differentiate real tokens from padding tokens with the ‚Äúattention mask‚Äù."]},{"cell_type":"markdown","metadata":{"id":"fZvQKKcxmu4L"},"source":["# 3.3. Tokenize Dataset\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"lSBvH82Amx_3","executionInfo":{"status":"ok","timestamp":1619565196269,"user_tz":-120,"elapsed":14895,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def prep_input(sentences,labels, max_len,tokenizer):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                           # Sentence to encode.\n","                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])       # IDs of the the vocabularies in the Model's dictionary\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors. \n","    input_ids = torch.cat(input_ids, dim=0)             # Concatenates the given sequence of seq tensors in the given dimension. \n","                                                        # All tensors must  have the same shape \n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids, attention_masks, labels\n","    else:\n","        return input_ids, attention_masks"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKdFMMmP8aT"},"source":["# 3.4. Importing Tokenizers and Input Preparation\n","\n","- Now it is time to import both Camembert and FlauBERT tokenizers from  pretained package and prepare the input using them. \n","\n","- Calling prep_input() for each model will result in the corresponding:\n","     \n","\n","1.   **input ids**\n","2.   **attention maks**\n","3.   **labels**\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BYzJChjw5AG","executionInfo":{"status":"ok","timestamp":1619565199455,"user_tz":-120,"elapsed":18059,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"586239c8-9c02-40f0-ca4f-2711c0295a1d"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAvaWiVEwq54","executionInfo":{"status":"ok","timestamp":1619565255676,"user_tz":-120,"elapsed":74252,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"79927a5b-812d-4795-9045-07bfa4ad785a"},"source":["input_ids_cam, attention_masks_cam, labels_cam = prep_input (sentences, labels, max_len, tokenizer_cam)\n"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-4ArTutx-Cx","executionInfo":{"status":"ok","timestamp":1619565425204,"user_tz":-120,"elapsed":243749,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"a46e7ab8-c194-4899-af76-70eb448583a0"},"source":["input_ids_flau, attention_masks_flau, labels_flau  = prep_input(sentences,labels, max_len,tokenizer_flau)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7XrGAY_zRX--"},"source":["# 3.5. Training & Validation Split\n","Divide up our training randomly select **10%** as a validation set off of the training set.\n","\n","While splitting, we used the following parameters:\n","\n","\n","1.   **stratify**: \n","in this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n","2.   **random_state**: \n","simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time."]},{"cell_type":"code","metadata":{"id":"nZQ0mB1NyeWa","executionInfo":{"status":"ok","timestamp":1619565425206,"user_tz":-120,"elapsed":243726,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["val_size = 0.1\n","\n","tr_inputs_cam, val_inputs_cam, _,_ = train_test_split (input_ids_cam, labels_cam, stratify = labels_cam,    \n","                                                            random_state=2020, test_size = val_size)\n","\n","tr_masks_cam, val_masks_cam, _,_ =   train_test_split (attention_masks_cam, labels, stratify = labels,        # labels: Preprocess.labels\n","                                                            random_state=2020, test_size = val_size)\n","\n","tr_inputs_flau, val_inputs_flau, _,_ = train_test_split (input_ids_flau, labels_flau, stratify=labels,\n","                                                            random_state=2020, test_size = val_size)\n","\n","tr_masks_flau, val_masks_flau, _,_   = train_test_split (attention_masks_flau, labels,stratify=labels_flau,  # labels: Preprocess.labels\n","                                                            random_state=2020, test_size = val_size)"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLOoMQfqryo8","executionInfo":{"status":"ok","timestamp":1619565425207,"user_tz":-120,"elapsed":243705,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# tr_inputs, test_inputs_cam, tr_labels, test_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.1)\n","\n","# tr_inputs_cam, val_inputs_cam, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.1)\n","\n","# tr_masks, test_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.1)\n","\n","# tr_masks_cam, val_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.1 )\n","\n","\n","\n","# tr_inputs, test_inputs_flau, tr_labels, test_labels_flau = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.1)\n","\n","# tr_inputs_flau, val_inputs_flau, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.1)\n","\n","# tr_masks, test_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.1)\n","\n","# tr_masks_flau, val_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.1 )"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiE2yDIzuRO","executionInfo":{"status":"ok","timestamp":1619565426887,"user_tz":-120,"elapsed":245370,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","\n","torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","torch.save(val_masks_flau, \"val_masks_flau.pt\")"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdkITHXh0Ahs","executionInfo":{"status":"ok","timestamp":1619565426892,"user_tz":-120,"elapsed":245360,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_input='./'\n","\n","tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","\n","tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1O4EyDzCpsS"},"source":["# 4. Defining Models to be Fused\n","\n","Now, as our  data has been preprocessed, cleaned and text data was tokenzied , it is ready to be fed to the models. \n","- As a first step,  first we need to define and configure the models. \n"]},{"cell_type":"markdown","metadata":{"id":"rlj0Dz1FW7cM"},"source":["# 4.1. RESNet Model for Image Processing. \n","\n","In PyTorch, you always need to define a forward method for your neural network model. But you never have to call it explicitly.\n","Here we are defining our image processing class is subclass of nn.Module and is inheriting all methods. In the super class, nn.Module, there is a __call__ method which obtains the forward function from the subclass and calls it."]},{"cell_type":"markdown","metadata":{"id":"rnnJlj2Y3iii"},"source":["# 4.1.1.  Image Processing Model - RESNet50\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QWvMo_nV1XCb","executionInfo":{"status":"ok","timestamp":1619565428471,"user_tz":-120,"elapsed":246926,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.nn import functional as F\n","import torch.nn as nn\n","import pretrainedmodels\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained='imagenet'):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                self.base_model.load_state_dict(torch.load(resnet_model_path))\n","                )\n","            \n","        self.l0 = nn.Linear(2048, 27)  # Applies a linear transformation to the incoming data\n","        # batch_size = 2048\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","\n","        # During the training you will get batches of images, \n","        # so your shape in the forward method will get an additional batch dimension at dim0: \n","        # [batch_size, channels, height, width].\n","        \n","        x = self.base_model.features(image) \n","\n","        #extracting feature vector from network after feature leaning \n","        #This is the flatten vector \n","\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1) \n","        #adaptive_avg_pool2d : Kernel size = (input_size+target_size-1) // target_size rounded up\n","        #Then the positions of where to apply the kernel are computed as rounded equidistant points between 0 and input_size - kernel_size\n","        \n","        out = self.l0(x)\n","\n","        return out"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaAYA3QY1cbO","executionInfo":{"status":"ok","timestamp":1619565428478,"user_tz":-120,"elapsed":246918,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class Identity(nn.Module):\n","  \n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr8ciZGg1Ee4"},"source":["# 4.1.2. Instaniating the Image Processing Network \n"," Now we create an instance from the SEResnext50_32x4d class that we defined and load the weights from a pretrained model, since the training is done previously. "]},{"cell_type":"code","metadata":{"id":"ZGGor-mS1zJe","executionInfo":{"status":"ok","timestamp":1619565428479,"user_tz":-120,"elapsed":246905,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#data_path = '/content/drive/My Drive/Rakuten/'\n","\n","img_model = SEResnext50_32x4d(pretrained=None)\n","# img_model.load_state_dict(torch.load(os.path.join(data_path, 'models/RESNET_best_model.pt')))\n","\n","# img_model.cuda()"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9gfXO9k21-o"},"source":["# 4.1.3. Prinitng Model's Params"]},{"cell_type":"code","metadata":{"id":"WOiwcopd8m8B","executionInfo":{"status":"ok","timestamp":1619565428481,"user_tz":-120,"elapsed":246893,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.l0 = Identity()"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb7I6qBq2c0E","executionInfo":{"status":"ok","timestamp":1619565428483,"user_tz":-120,"elapsed":246873,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5eeec6dd-6742-4e93-c2b3-3e4a5ea5339c"},"source":["for param in img_model.parameters():\n","     print(type(param), param.size())"],"execution_count":55,"outputs":[{"output_type":"stream","text":["<class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 7, 7])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000, 2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9iqaXH5A3Esl"},"source":["# 4.1.4. Model's Params Require No Grads\n","\n","These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","As our model is already trained and weights are assigned, then there is no need to calculate the gradients so no need to send them to the optimizer."]},{"cell_type":"code","metadata":{"id":"c0XJcLgW8u09","executionInfo":{"status":"ok","timestamp":1619565428484,"user_tz":-120,"elapsed":246845,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# params are iterators which contain model's parameters. Usually passed to the optimizer\n","for params in img_model.parameters():\n","      params.requires_grad = False"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXULFfzkdB8","executionInfo":{"status":"ok","timestamp":1619565428485,"user_tz":-120,"elapsed":246833,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.out_proj = Identity()"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWgGQqbU4nNR"},"source":["# Image Data Preparation"]},{"cell_type":"code","metadata":{"id":"4yNKxOJ2-QrA","executionInfo":{"status":"ok","timestamp":1619565428487,"user_tz":-120,"elapsed":246820,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Data path\n","# text_data_path = os.path.join('/content/drive/My Drive/Rakuten')\n","# image_data_path = os.path.join('')\n"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM6bWAoKTsx7","executionInfo":{"status":"ok","timestamp":1619565428488,"user_tz":-120,"elapsed":246806,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def get_img_path(img_id, prd_id, path):\n","    \n","    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n","    return path + pattern"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRCBjDt14urB","executionInfo":{"status":"ok","timestamp":1619565428489,"user_tz":-120,"elapsed":246785,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"975813d6-3113-497e-e264-6f21d0c1f4a4"},"source":["print(type(X_test))"],"execution_count":60,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VL-Ubip6Jhld","executionInfo":{"status":"ok","timestamp":1619565428491,"user_tz":-120,"elapsed":246765,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["X_test = pd.read_csv(text_data_path + \"/NewTest.csv\")"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNt322m5Jvxi","executionInfo":{"status":"ok","timestamp":1619565428492,"user_tz":-120,"elapsed":246741,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5e3d9790-1072-4238-ca4b-3b70d8a823e5"},"source":["print(type(X_test))"],"execution_count":62,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYomnrt7wKWw","executionInfo":{"status":"ok","timestamp":1619565428494,"user_tz":-120,"elapsed":246710,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9ceb40da-b3f2-4e73-d98b-7ec468a75a0b"},"source":["print(len(Preprocess.train), len(train))"],"execution_count":63,"outputs":[{"output_type":"stream","text":["55025 55025\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QzgyQu0aY17J"},"source":["# Obtaining & Splitting Images "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["572320394c06443fb199666934c62dc8","1627f3db2fba4a81be8894debef064d5","bf79d63cdbe04652b101ae9c5d9a7b2a","fef488bfaf3040ba943b8285d6eab717","2ece545f40f145a5a5138fb6d633ad92","5ea811bc2f544d24b7b5d917e8bbadf1","01945d169fff46419f65ba37ecf3bc6e","cf418a52fbca4f2f818a5c5b0de049bf","95a1b6564d5143e89d620c6c89107de7","0866b5e75224494d975a3444d9bae164","bc7b205d364c43f4a675afe3ddc7eec1","b4dc3e261b2844668c802f759d895917","87479667567347689e0e5a0188f0b6b7","391cb815527b4172b316afd5b09c7b04","5eadea8c77d54e168fbc78d76c705283","8a3e99e18f304573b7acc63fefcdd9d0"]},"id":"emmAR45klxoi","executionInfo":{"status":"ok","timestamp":1619565429987,"user_tz":-120,"elapsed":248174,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b47e1297-10c1-4f09-d039-0eab42b0531a"},"source":["train_img = train[['Image_id','Product_id','labels','product']]\n","\n","train_img['image_path'] = Preprocess.train.progress_apply(lambda x: get_img_path(x['Image_id'], x['Product_id'],\n","                                                      path = os.path.join(image_data_path, 'image_training/')),axis=1)\n","\n","X_test['image_path'] = X_test.progress_apply(lambda x: get_img_path(x['Image_id'],x['Product_id'],\n","                                                      path= os.path.join(image_data_path, 'image_test/image_test_task1_phase1/')),axis=1)\n","\n","train_df, val_df, tr_labels, test_labels = train_test_split(train_img, train_img['labels'], \n","                                           random_state=2020,\n","                                           test_size = 0.1,\n","                                           stratify=train_img['labels'])\n","\n","\n","# train_df, val_df, train_labels, val_labels = train_test_split(tr_df, tr_labels, \n","#                                            random_state=2020,\n","#                                            test_size = 0.1,\n","#                                            stratify=tr_labels)"],"execution_count":64,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"572320394c06443fb199666934c62dc8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=55025.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95a1b6564d5143e89d620c6c89107de7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=597.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_h5_e_Gw-M9","executionInfo":{"status":"ok","timestamp":1619565429988,"user_tz":-120,"elapsed":248143,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"08a383c5-3b55-4835-c86c-7f35525f762a"},"source":["print(\"Original Images Df:   \",  len(train_img))\n","print(\"Train Images DF:      \" , len(train_df))\n","print(\"Validation Images DF: \",  len(val_df))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Original Images Df:    55025\n","Train Images DF:       49522\n","Validation Images DF:  5503\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aIAzQAsCTzBs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565429990,"user_tz":-120,"elapsed":248115,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5f6ab883-a994-45a3-8752-b6a87408e3a2"},"source":["print (train_img['image_path'][0])\n"],"execution_count":66,"outputs":[{"output_type":"stream","text":["/content/Rakuten/image/image_training/image_938777978_product_201115110.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lRigVJwVBOcz"},"source":["# Image Data Augmentation\n","\n","We're going to be making use of Pytorch's transforms for preparing the input images to be used by our model. \n","\n","\n","\n","\n","\n","*   We'll need to make sure the images in the training set and validation set are the same size, so we'll be using transforms.Resize\n","*   We'll also be doing a little data augmentation, trying to improve the performance of our model by forcing it to learn about images at different angles and crops, so we'll randomly crop and rotate the images.\n","\n","*    we'll make tensors out of the images, as PyTorch works with tensors. \n","*   Finally, we'll normalize the images, which helps the network work with values that may be have a wide range of different values.\n","\n","\n","*   We then compose all our chosen transforms.\n","\n","It worth mentioning that validation transforms don't have any of the flipping or rotating, as they aren't part of our training set, so the network isn't learning about them\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-Togfn7XmpW5","executionInfo":{"status":"ok","timestamp":1619565429991,"user_tz":-120,"elapsed":248093,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["input_size = 224 # for Resnt\n","\n","# Applying Transforms to the Data\n","\n","from torchvision import datasets, models, transforms\n","\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nB9-K4lUNoqT"},"source":["# Text Processing Models - BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"markdown","metadata":{"id":"I_vkDiuRC70z"},"source":["# 4.2 CamemBERT Model"]},{"cell_type":"code","metadata":{"id":"imOtgakyCtFe","executionInfo":{"status":"ok","timestamp":1619565429992,"user_tz":-120,"elapsed":248080,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","  \n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            head_mask = head_mask,\n","            inputs_embeds=inputs_embeds,\n","#           output_attentions=output_attentions,\n","#           output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = outputs[0] #(B,256,768)\n","\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","\n","#       x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","\n","        x = self.dense(x)  # 768 -> 768\n","\n","        feat= torch.tanh(x) \n","\n","        logits = self.out_proj(feat) # 768 -> 27\n","\n","        outputs = (logits,) + outputs[2:] #3rd element onwards\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtPoFtaqDAj6"},"source":["# FlauBERT Model"]},{"cell_type":"code","metadata":{"id":"a16smoYhDCmn","executionInfo":{"status":"ok","timestamp":1619565429993,"user_tz":-120,"elapsed":248066,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["num_classes = 27\n","\n","class vec_output_FlaubertForSequenceClassification(FlaubertModel):\n","    \n","    config_class = FlaubertConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = FlaubertModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.init_weights()\n","        self.dropout =  torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        langs=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        lengths=None,\n","        cache=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        \n","        \n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            langs = langs,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            lengths = lengths,\n","            cache = cache,\n","            head_mask = head_mask,\n","            inputs_embeds = inputs_embeds,\n","        )\n","\n","        #output = self.dropout(output)\n","        output = transformer_outputs[0] \n","        vec = output[:,0]\n","        \n","        \n","        #logits\n","        dense = self.dropout(vec)\n","        \n","        #classifier\n","        logits = self.classifier(dense)\n","        \n","        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n","       \n","        \n","        return outputs,dense"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2krGO8BnVfd"},"source":["# Dataset Fusion"]},{"cell_type":"code","metadata":{"id":"qIIQ5-g3gU85","executionInfo":{"status":"ok","timestamp":1619565429994,"user_tz":-120,"elapsed":248051,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# TODO DELELTE IMAGES WITH NO DESCRIPTION\n","# From the preprocesssed file"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRq24YrsnU9X","executionInfo":{"status":"ok","timestamp":1619565429995,"user_tz":-120,"elapsed":248035,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","class FusionDataset(Dataset):\n","    \n","    def __init__(self, df, inputs_cam, masks_cam, inputs_flau, masks_flau, transform=None, mode='train'):\n","        self.df = df\n","        self.transform   = transform\n","        self.mode = mode\n","\n","        self.inputs_cam  = inputs_cam\n","        self.masks_cam   = masks_cam\n","\n","        self.inputs_flau  = inputs_flau\n","        self.masks_flau   = masks_flau\n","         \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        \n","        im_path = self.df.iloc[idx]['image_path']\n","        img= plt.imread(im_path)\n","        #img = cv2.imread(im_path)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        img              = img.cuda()\n","        input_id_cam     = self.inputs_cam[idx].cuda()\n","        input_mask_cam   = self.masks_cam[idx].cuda()\n","        input_id_flau    = self.inputs_flau[idx].cuda()\n","        input_mask_flau  = self.masks_flau[idx].cuda()\n","        \n","        if self.mode =='test':\n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","            \n","        else:\n","            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5RRyCzd4XSd","executionInfo":{"status":"ok","timestamp":1619565429995,"user_tz":-120,"elapsed":248020,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["a1 = torch.randn(3,10,10)"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_51PvBY4Zpr","executionInfo":{"status":"ok","timestamp":1619565429996,"user_tz":-120,"elapsed":248004,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["reduce_dim = nn.Conv1d(in_channels = 10 , out_channels = 1 , kernel_size= 1)"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wryxtip-4b0m","executionInfo":{"status":"ok","timestamp":1619565429997,"user_tz":-120,"elapsed":247983,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7fcde766-09e8-4562-ea21-3a1fca2e4146"},"source":["reduce_dim(a1).view(3,10).shape"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 10])"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTUa-CbEnluo","executionInfo":{"status":"ok","timestamp":1619565436322,"user_tz":-120,"elapsed":254278,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"87934119-5ed4-407e-db7a-be3504327a22"},"source":["print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","\n","input_ids_test_flau,attention_masks_test_flau = prep_input(test_sentences, labels=None, max_len=max_len,tokenizer = tokenizer_flau)\n","\n","input_ids_test_cam,attention_masks_test_cam = prep_input(test_sentences , labels=None, max_len=max_len,tokenizer = tokenizer_cam)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8gijJIM5pQX","executionInfo":{"status":"ok","timestamp":1619565436324,"user_tz":-120,"elapsed":254246,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4ece533e-bb90-4af1-812d-6ee423e4f497"},"source":["print(type(Preprocess.test_sentences))\n","print(len(Preprocess.test_sentences))"],"execution_count":76,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","597\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HXbLhSkyCilV","executionInfo":{"status":"ok","timestamp":1619565436325,"user_tz":-120,"elapsed":254223,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Moodels path \n","# resnet_model_path = '/content/Rakuten/models/RESNET_best_model.pt'\n","# camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_title_description.pt' ###### TODO Change with the updated model!!!\n","# flaubert_model_path = '/content/Rakuten/models/FlauBERT_best_model_title_description.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'\n"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzlptrSui4af","executionInfo":{"status":"ok","timestamp":1619565436326,"user_tz":-120,"elapsed":254207,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Moodels path \n","resnet_model_path = '/content/Rakuten/models/90_10_RESNET_model.pt'\n","\n","camembert_model_path_title = '/content/Rakuten/models/90_10_Hirachical_CamemBERT_title.pt'\n","camembert_model_path_desc = '/content/Rakuten/models/90_10_Hirachical_CamemBERT_description.pt'\n","\n","flaubert_model_path_title = '/content/Rakuten/models/FlauBERT_best_title.pt'\n","flaubert_model_path_desc = '/content/Rakuten/models/90_10_Hirachical_FlauBERT_description.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'"],"execution_count":78,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09NiYnUxfi94"},"source":["# Fuse\n"," When using pretrained models, PyTorch sets the model to be unfrozen (will have its weights adjusted) by default"]},{"cell_type":"code","metadata":{"id":"TxW8Ups_nr8O","executionInfo":{"status":"ok","timestamp":1619565436328,"user_tz":-120,"elapsed":254193,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vector_fusion(nn.Module):    \n","    def __init__(self):\n","        super(vector_fusion, self).__init__()\n","\n","        self.img_model = SEResnext50_32x4d(pretrained=None)\n","        self.img_model.load_state_dict(torch.load(resnet_model_path))\n","        self.img_model.l0=Identity()\n","        for params in self.img_model.parameters():\n","            params.requires_grad=False\n","\n","# ------ CamamBERT ------\n","\n","        self.cam_model_title = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_title)\n","        self.cam_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_title.out_proj = Identity()\n","\n","\n","        self.cam_model_desc = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_desc)\n","        self.cam_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_desc.out_proj = Identity()\n","\n"," # ----  FlauBERT ----- \n","\n","        \n","        self.flau_model_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_title)\n","\n","        self.flau_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_title.classifier=Identity()\n","\n","\n","      \n","        self.flau_model_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_desc)\n","\n","        self.flau_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_desc.classifier=Identity()\n","\n","\n","# ------------------------------------------------------------\n","\n","        self.reduce_dim = nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n","        self.reduce_dim2 = nn.Conv1d(in_channels = 768 , out_channels = 1 , kernel_size= 1)\n","        self.out = nn.Linear(768, 27)\n","        \n","        #gamma\n","#         self.w1 = nn.Parameter(torch.zeros(1))\n","#         self.w2 = nn.Parameter(torch.zeros(1))\n","#         self.w3 = nn.Parameter(torch.zeros(1))\n","        \n","    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n","        \n","        cam_emb_title,vec1_title = self.cam_model_title(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","        cam_emb_desc,vec1_desc = self.cam_model_desc(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","\n","#---------------------------------\n","        \n","        flau_emb_title,vec2 =self.flau_model_title(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","        flau_emb_desc,vec2_desc =self.flau_model_desc(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","# ---------------------------------\n","        \n","        #Projecting the image embedding to lower dimension\n","        img_emb = self.img_model(img)\n","        \n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1],1)\n","        img_emb = self.reduce_dim(img_emb)\n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n","# --------------------------------\n","\n","        #summing up the vectors\n","        cam_emb  = cam_emb_title + cam_emb_desc\n","        flau_emb = flau_emb_title + flau_emb_desc\n","        \n","        #Bilinear\n","        #text_emb = text_emb.view(text_emb.shape[0],1,text_emb.shape[1])  ##### bs * 1 * 768\n","        \n","        #Bilinear Pooling\n","        #pool_emb = torch.bmm(img_emb,text_emb) ### bs * 768 * 768\n","        #pool_emb = self.reduce_dim2(pool_emb).view(text_emb.shape[0],768)  #### bs * 1 * 768\n","\n","        fuse= img_emb + cam_emb[0] + flau_emb[0]\n","\n","        # print(\"fusion size\", fuse.shape)  # 128\n","        \n","        logits = self.out(fuse)\n","\n","        # print(\"returned logits shape: \", logits.shape)\n","        return logits"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1VU1vrSQFzD","executionInfo":{"status":"ok","timestamp":1619565436328,"user_tz":-120,"elapsed":254177,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#  img_model = SEResnext50_32x4d(pretrained=None)\n","#  img_model.load_state_dict(torch.load(resnet_model_path))"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4LM7TANsMqJ","executionInfo":{"status":"ok","timestamp":1619565436329,"user_tz":-120,"elapsed":254164,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cam_title= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_title)\n","# cam_model.load_state_dict(checkpoint)"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0iQCIXYsTlf","executionInfo":{"status":"ok","timestamp":1619565436330,"user_tz":-120,"elapsed":254149,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cam_desc= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_desc)\n","# cam_model.load_state_dict(checkpoint)"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZfncq4PO_EY","executionInfo":{"status":"ok","timestamp":1619565436331,"user_tz":-120,"elapsed":254134,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_title)\n","# flau_title.load_state_dict(checkpoint)"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo090Yeusw5V","executionInfo":{"status":"ok","timestamp":1619565436332,"user_tz":-120,"elapsed":254122,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_desc)\n","# flau_desc.load_state_dict(checkpoint)"],"execution_count":84,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eU5pHNZKNKr3"},"source":["#  Instantiation  & Training of Fusion Model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Qpj2jQYu0P-","executionInfo":{"status":"ok","timestamp":1619565478806,"user_tz":-120,"elapsed":296581,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d9e1dc03-e109-43b7-b868-983c66f650a5"},"source":["model = vector_fusion() "],"execution_count":85,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GN9HIANRu1_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565480606,"user_tz":-120,"elapsed":298358,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ceb2952b-571c-43d6-ce47-eaa32f00582a"},"source":["model.cuda()"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["vector_fusion(\n","  (img_model): SEResnext50_32x4d(\n","    (base_model): SENet(\n","      (layer0): Sequential(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      )\n","      (layer1): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (4): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (5): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (avg_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (last_linear): Linear(in_features=2048, out_features=1000, bias=True)\n","    )\n","    (l0): Identity()\n","  )\n","  (cam_model_title): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (cam_model_desc): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (flau_model_title): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (flau_model_desc): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (reduce_dim): Conv1d(2048, 768, kernel_size=(1,), stride=(1,))\n","  (reduce_dim2): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n","  (out): Linear(in_features=768, out_features=27, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"wbah-djKPyRB"},"source":["# Fuse Input Data"]},{"cell_type":"code","metadata":{"id":"ZZBQFQnFSn6L","executionInfo":{"status":"ok","timestamp":1619565480608,"user_tz":-120,"elapsed":298337,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["train_dataset = FusionDataset (train_df, tr_inputs_cam, tr_masks_cam, tr_inputs_flau, tr_masks_flau,\n","                            transform = image_transforms['train'])\n","\n","val_dataset = FusionDataset (val_df, val_inputs_cam, val_masks_cam, val_inputs_flau, val_masks_flau,\n","                          transform = image_transforms['valid'])\n","\n","# test_dataset = FusionDataset (X_test, input_ids_test_cam, attention_masks_test_cam, input_ids_test_flau, attention_masks_test_flau\n","#                            , transform = image_transforms['test'],mode = 'test')"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPY9mTkrs2Jl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565480609,"user_tz":-120,"elapsed":298321,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"77e4203d-f2b2-457d-eff3-8c9b8a459c7a"},"source":["print(len(train_df), len(tr_inputs_cam), len(tr_inputs_flau))"],"execution_count":88,"outputs":[{"output_type":"stream","text":["49522 49522 49522\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VXWSFsjKVnXn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565480610,"user_tz":-120,"elapsed":298298,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9c183070-bc62-42e3-e4ee-bf26711b9ea5"},"source":["print(len(train_dataset))"],"execution_count":89,"outputs":[{"output_type":"stream","text":["49522\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uh2PccgCXJeI"},"source":["# Data Loaders\n","\n","We need to use the DataLoaders to create iterable objects for us to work with. We tell it which datasets we want to use, give it a batch size, and shuffle the data"]},{"cell_type":"code","metadata":{"id":"nfFhYJYpSr05","executionInfo":{"status":"ok","timestamp":1619565480610,"user_tz":-120,"elapsed":298274,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["batch_size = 128 #increase batch size to reduce the noise \n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","validation_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n"," \n","# test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-gP7nVnSwAt","executionInfo":{"status":"ok","timestamp":1619565480611,"user_tz":-120,"elapsed":298258,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["optimizer = AdamW( model.parameters(),\n","                  lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n","                  weight_decay= 0.001\n","                )"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgoWU2g6S1CZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619565480612,"user_tz":-120,"elapsed":298243,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"03d1ce7f-f562-44bc-a7ad-62e479c60312"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","count_parameters(model)"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1595164"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"id":"0CRNzbwCS6vU","executionInfo":{"status":"ok","timestamp":1619572677730,"user_tz":-120,"elapsed":765,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 10\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"id":"en6CE-_8S-Vw","executionInfo":{"status":"ok","timestamp":1619565480613,"user_tz":-120,"elapsed":298177,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import torch.nn as nn\n","loss_criterion = nn.CrossEntropyLoss()"],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLg8d2UmTBLp","executionInfo":{"status":"ok","timestamp":1619565480614,"user_tz":-120,"elapsed":298158,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY7xbQXqTHUF","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1619589142530,"user_tz":-120,"elapsed":1284999,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4cad0694-01ba-4dba-c8bd-61f1eb5270ec"},"source":["from sklearn.metrics import f1_score\n","\n","seed_val = 42\n","# seed_val = 50\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","train_loss_values = []\n","\n","val_loss_values = []\n","logits_values =[]\n","\n","############\n","\n","total_train_accuracy = 0\n","avg_train_accuracy = 0\n","\n","train_accuracy_values = []\n","val_accuracy_values = []\n","\n","##########\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    \n","    #tr and val\n","#     vec_output_tr = []\n","#     vec_output_val =[]\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","    total_train_accuracy = 0\n","    predictions=[]\n","    true_labels=[]\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","\n","    model.to(device)\n","    best_f1 = 0\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in (enumerate(train_dataloader)):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #   \n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","#         return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","\n","        b_img               = batch[0].to(device)\n","        b_input_id_cam      = batch[1].to(device)\n","        b_input_mask_cam    = batch[2].to(device)\n","        b_input_id_flau     = batch[3].to(device)\n","        b_input_mask_flau   = batch[4].to(device)\n","        b_labels            = batch[5].to(device)\n","        \n","        \n","        model.zero_grad()    #set the gradients to zero before starting to do backpropragation because PyTorch accumulates \n","                            # the gradients on subsequent backward passes\n","\n","        \n","        logits = model(b_img, b_input_id_cam , b_input_mask_cam, b_input_id_flau, b_input_mask_flau)  # 27\n","\n","        # print(\"logits shape: \", logits.shape)   \n","        # print(\"b_labels shape: \", b_labels.shape)\n","        # print(logits) \n","        # print(b_labels) \n","                            \n","        #Defining the loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_tr.extend(vec)\n","        \n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","\n","        total_train_loss += loss.item()\n","#-------------------------------------------------------\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","#-------------------------------------------------------\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","        \n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","\n","    print(\"\")\n","    print(\"Training Accuracy: {}\".format(avg_train_accuracy))\n","    train_accuracy_values.append(avg_train_accuracy)\n","\n","    ######################################################################################\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)  \n","    train_loss_values.append(avg_train_loss) \n","             \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    \n","    print(\"  Average training loss: {} \".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:} \".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    \n","\n","    # Evaluate data for one epoch\n","    for batch in (validation_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_val.extend(vec)\n","        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","#--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","#------------------------------\n","    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    print(\"Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","    if curr_f1 > best_f1:\n","        best_f1=curr_f1\n","        torch.save(model.state_dict(), '/content/drive/My Drive/Rakuten/models/90_10_SUM_Hirarical.pt')\n","#         np.save('best_vec_train_model_train.npy',vec_output_tr)\n","#         np.save('best_vec_val.npy',vec_output_val)\n","        \n","    # Record all statistics from this epoch.\n","#     training_stats.append(\n","#         {\n","#             'epoch': epoch_i + 1,\n","#             'Training Loss': avg_train_loss,\n","#             'Valid. Loss': avg_val_loss,\n","#             'Valid. Accur.': avg_val_accuracy,\n","#             'Training Time': training_time,\n","#             'Validation Time': validation_time\n","#         }\n","#     )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","print()\n","\n","plt.plot(np.array(train_loss_values), 'r', label='Train Loss')\n","plt.plot(np.array(val_loss_values), 'b', label='Val Loss'  )\n","plt.legend()\n","plt.title('Model Loss')\n","plt.show()\n","\n","print()\n","\n","plt.plot(np.array(train_accuracy_values), 'r', label='Train Accuracy')\n","plt.plot(np.array(val_accuracy_values), 'b', label='Val Accuracy'  )\n","plt.legend()\n","plt.title('Model Accuracy')\n","plt.show()\n","\n","#print(logits_values)\n"],"execution_count":98,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9898255813953488\n","  Average training loss: 0.030014527739610438 \n","  Training epcoh took: 0:24:08 \n","\n","Running Validation...\n","  Accuracy: 0.9254929843435268\n","  Validation Loss: 0.34978115662585857\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9190452829601808\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9902218907248742\n","  Average training loss: 0.027892308306666114 \n","  Training epcoh took: 0:24:02 \n","\n","Running Validation...\n","  Accuracy: 0.9251296122505035\n","  Validation Loss: 0.35503793074641116\n","  Validation took: 0:02:27\n","Validation F1-Score: 0.919937169662603\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9909560723514211\n","  Average training loss: 0.026479498104213975 \n","  Training epcoh took: 0:24:00 \n","\n","Running Validation...\n","  Accuracy: 0.9269479033144112\n","  Validation Loss: 0.3590863832207613\n","  Validation took: 0:02:29\n","Validation F1-Score: 0.9231980937808515\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9916374835667982\n","  Average training loss: 0.02490871820962225 \n","  Training epcoh took: 0:23:51 \n","\n","Running Validation...\n","  Accuracy: 0.9282197056399926\n","  Validation Loss: 0.35563700039719426\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9227742421647649\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.991556734212793\n","  Average training loss: 0.025331509202413106 \n","  Training epcoh took: 0:23:38 \n","\n","Running Validation...\n","  Accuracy: 0.9273112754074344\n","  Validation Loss: 0.3572518056215242\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9220691474354498\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9909535932045878\n","  Average training loss: 0.025233269848401154 \n","  Training epcoh took: 0:23:47 \n","\n","Running Validation...\n","  Accuracy: 0.9278563335469694\n","  Validation Loss: 0.35983675960884537\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9237065860957487\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9917433785529716\n","  Average training loss: 0.024169334373196218 \n","  Training epcoh took: 0:24:22 \n","\n","Running Validation...\n","  Accuracy: 0.9274929614539461\n","  Validation Loss: 0.3560884508282639\n","  Validation took: 0:02:30\n","Validation F1-Score: 0.9245169591354402\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9919250645994832\n","  Average training loss: 0.023154449155217263 \n","  Training epcoh took: 0:24:21 \n","\n","Running Validation...\n","  Accuracy: 0.9273112754074344\n","  Validation Loss: 0.3570220612509306\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9243351426439764\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9926316214470284\n","  Average training loss: 0.02175631882798514 \n","  Training epcoh took: 0:24:21 \n","\n","Running Validation...\n","  Accuracy: 0.9284013916865043\n","  Validation Loss: 0.3581233804309091\n","  Validation took: 0:02:29\n","Validation F1-Score: 0.9238530698910078\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","Training Accuracy: 0.9920638968221588\n","  Average training loss: 0.022488802105148663 \n","  Training epcoh took: 0:24:13 \n","\n","Running Validation...\n","  Accuracy: 0.9278563335469694\n","  Validation Loss: 0.3580431148063305\n","  Validation took: 0:02:28\n","Validation F1-Score: 0.9241100978275877\n","\n","Training complete!\n","Total training took 4:34:18 (h:mm:ss)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"A_lgPHpW0kv3","executionInfo":{"status":"aborted","timestamp":1619572674979,"user_tz":-120,"elapsed":7492431,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["model_path =  '/content/drive/My Drive/Rakuten/models/90_10_SUM_Hirarical.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZFcFqewuNw0","executionInfo":{"status":"aborted","timestamp":1619572674981,"user_tz":-120,"elapsed":7492410,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# checkpoint = torch.load(model_path)\n","# model.load_state_dict(checkpoint) # A state_dict is simply a Python dictionary object \n","#                                   # that maps each layer to its parameter tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVr9pt2PuQLa","executionInfo":{"status":"aborted","timestamp":1619572674982,"user_tz":-120,"elapsed":7492390,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# import matplotlib.pyplot as plt\n","# from sklearn.metrics import f1_score\n","\n","# def predict_pyt(model, prediction_dataloader):\n","#     \"\"\"\n","#     model: pytorch model\n","#     prediction_dataloader: DataLoader object for which the predictions has to be made.\n","#     return:\n","#         predictions:    - Direct predicted labels\n","#         softmax_logits: - logits which are normalized with softmax on output\"\"\"\n","    \n","    \n","#     print(\"\")\n","#     print(\"Running Testing...\")\n","\n","#     t0 = time.time()\n","\n","#     # Put the model in evaluation mode--the dropout layers behave differently\n","#     # during evaluation.\n","#     model.eval()\n","\n","#     # Tracking variables \n","#     total_eval_accuracy = 0\n","#     total_eval_loss = 0\n","#     nb_eval_steps = 0\n","#     predictions=[]\n","#     true_labels=[]\n","#     logits_values =[]\n","#     val_accuracy_values = []\n","#     val_loss_values = []\n","    \n","    \n","#     total_t0 = time.time()\n","#     # Evaluate data for one epoch\n","#     for batch in (prediction_dataloader):\n","        \n","#         # Unpack this training batch from our dataloader. \n","#         #\n","#         # As we unpack the batch, we'll also copy each tensor to the GPU using \n","#         # the `to` method.\n","#         #\n","#         # `batch` contains three pytorch tensors:\n","#         #   [0]: input ids \n","#         #   [1]: attention masks\n","#         #   [2]: labels \n","        \n","#         b_img = batch[0].to(device)\n","\n","#         b_input_id_cam = batch[1].to(device)\n","#         b_input_mask_cam = batch[2].to(device)\n","#         b_input_id_flau = batch[3].to(device)\n","#         b_input_mask_flau = batch[4].to(device) \n","#         b_labels = batch[5].to(device)\n","        \n","        \n","#         # Tell pytorch not to bother with constructing the compute graph during\n","#         # the forward pass, since this is only needed for backprop (training).\n","#         with torch.no_grad():       \n","        \n","\n","#             # Forward pass, calculate logit predictions.\n","#             # token_type_ids is the same as the \"segment ids\", which \n","#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n","#             # The documentation for this `model` function is here: \n","#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","#             # Get the \"logits\" output by the model. The \"logits\" are the output\n","#             # values prior to applying an activation function like the softmax.\n","\n","\n","#             logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","#         #new\n","        \n","#         #defining the val loss\n","#         loss = loss_criterion(logits, b_labels)\n","        \n","#         # Accumulate the validation loss.\n","#         total_eval_loss += loss.item()\n","\n","#         # Move logits and labels to CPU\n","#         logits = logits.detach().cpu().numpy()\n","\n","#         # Move logits and labels to CPU\n","#         predicted_labels=np.argmax(logits,axis=1)\n","#         predictions.extend(predicted_labels)\n","#         label_ids = b_labels.to('cpu').numpy()\n","#         true_labels.extend(label_ids)\n","\n","#         ##########################################################################\n","\n","#         logits_values.append(predicted_labels)\n","\n","#         ##########################################################################\n","\n","#         # Calculate the accuracy for this batch of test sentences, and\n","#         # accumulate it over all batches.\n","#         total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","#     # Report the final accuracy for this validation run.\n","#     avg_val_accuracy = total_eval_accuracy / len(prediction_dataloader)\n","#     val_accuracy_values.append(avg_val_accuracy)\n","# #--------------------------------\n","#     print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","#     # Calculate the average loss over all of the batches.\n","#     avg_val_loss = total_eval_loss / len(prediction_dataloader)\n","# #-----------------------------\n","#     val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","#     # Measure how long the validation run took.\n","#     validation_time = format_time(time.time() - t0)\n","    \n","#     print(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\n","#     print(\"  Test took: {:}\".format(validation_time))\n","#     print(\"Test F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","#     curr_f1=f1_score(true_labels,predictions,average='macro')\n","\n","\n","#     print(\"\")\n","#     print(\"Testing complete!\")\n","\n","#     #print(\"Total testing took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","#     print()\n","#     plt.plot(np.array(val_accuracy_values), 'r', label='Test accuracy')\n","#     plt.legend()\n","#     plt.title('Test Curve')\n","#     plt.show()\n","\n","#     print()\n","#     print('DONE')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te03NzKu28Fs","executionInfo":{"status":"aborted","timestamp":1619572674983,"user_tz":-120,"elapsed":7492367,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# predict_pyt(model, test_dataloader)"],"execution_count":null,"outputs":[]}]}