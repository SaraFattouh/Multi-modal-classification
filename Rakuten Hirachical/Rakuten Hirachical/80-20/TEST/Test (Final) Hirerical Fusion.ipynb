{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test (Final) Hirerical Fusion.ipynb","provenance":[{"file_id":"16T2J5BOltd1RT58Mr4O81xPwRckKNTvO","timestamp":1619271008853},{"file_id":"10HG_Jj6e-CNcUe_Ng3H6hAbd0crPZTLT","timestamp":1618388020139},{"file_id":"1Iwp1sNEfG6ly73_MK_bi3EUEZAC05-lQ","timestamp":1617026135135},{"file_id":"1OCoA3nYHVEcUF5r3Sxaq9PuAqT1U9Lx7","timestamp":1616632198364},{"file_id":"1GEPelG8M4pwx_llUYLlmUO6jfjnUgMPl","timestamp":1616587813712},{"file_id":"1Px7sog6jBh8jhBFgrHAKeqt1maPE48jM","timestamp":1614749164976}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1fCPKaFCgw23"},"source":["# Multimodal Calssification on Rakuten France Dataset\n","# Multi Modal Addition Fusion"]},{"cell_type":"code","metadata":{"id":"feRlWWySuDCt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620049024639,"user_tz":-120,"elapsed":2671,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3c4a38d2-e158-458d-bc90-f7bfc8211fb7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZouy12lm_Xv","executionInfo":{"status":"ok","timestamp":1620049024640,"user_tz":-120,"elapsed":2657,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !ls"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"oG6LhxvoLTKt","executionInfo":{"status":"ok","timestamp":1620049024641,"user_tz":-120,"elapsed":2646,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhBoZ1r7tBSd","executionInfo":{"status":"ok","timestamp":1620049024641,"user_tz":-120,"elapsed":2634,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir Rakuten"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtpdRXBytE0X","executionInfo":{"status":"ok","timestamp":1620049024642,"user_tz":-120,"elapsed":2628,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cd './Rakuten'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Im900y65tWcT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620049024643,"user_tz":-120,"elapsed":2618,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"62eaa572-8011-4a80-9353-5003b5d8e418"},"source":["mkdir models data "],"execution_count":9,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory â€˜modelsâ€™: File exists\n","mkdir: cannot create directory â€˜dataâ€™: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jzp_vuw1MuNS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1620049024649,"user_tz":-120,"elapsed":2616,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f48e032c-8496-4e5b-e04a-bec1242e236f"},"source":["pwd"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/Rakuten'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"pxUWT_9hzEVv","executionInfo":{"status":"ok","timestamp":1620049024650,"user_tz":-120,"elapsed":2604,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_title.pt' models"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwsYn7UatdVV","executionInfo":{"status":"ok","timestamp":1620049024650,"user_tz":-120,"elapsed":2595,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_description.pt' models\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFVZ-GX6SDCp","executionInfo":{"status":"ok","timestamp":1620049024651,"user_tz":-120,"elapsed":2589,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_description.pt' models"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrrpaqRTui48","executionInfo":{"status":"ok","timestamp":1620049024651,"user_tz":-120,"elapsed":2583,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_title.pt' models\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEeo6IJHukVz","executionInfo":{"status":"ok","timestamp":1620049024652,"user_tz":-120,"elapsed":2577,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/Final_RESNet_model.pt' models"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEebtBL70gi6","executionInfo":{"status":"ok","timestamp":1620049024652,"user_tz":-120,"elapsed":2569,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTwB_kSusK2"},"source":["!cp  '/content/drive/My Drive/Rakuten/image.zip' './'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iY2GDPw1MZh"},"source":["ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhdiQHVFvSj3"},"source":["!unzip  ./image.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWx79doRO-pp"},"source":["pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1kbpdFQwrir"},"source":[" !cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/X_train.tsv' data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USx_EdDXMeU4"},"source":["pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXpLoRd1h5xu"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"FIGO5jehh8-2"},"source":["# 1.1 Using Colab GPU for Training\n","\n","Since weâ€™ll be training a large neural network itâ€™s best to take advantage of the free GPUs and TPUs that Google offers (in this case weâ€™ll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)"]},{"cell_type":"code","metadata":{"id":"-hgdtmoPf2al"},"source":["import os, time, datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import random\n","import logging\n","tqdm.pandas()\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","#NN Packages\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n","\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEwI57OheXSi"},"source":["torch.manual_seed(123)\n","torch.cuda.manual_seed(123)\n","torch.backends.cudnn.enabled=False\n","torch.backends.cudnn.deterministic=True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-DN_gCoh_pG"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dnUWVTZM8Wc"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tibnb-j2mVPo"},"source":["# 1.2. Installing the Hugging Face Library - Image Pretrained Models\n","\n","Install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","Weâ€™ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but donâ€™t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using.\n","\n","The goal  of the **pretrainedmodels** is to:\n","\n","-  help to reproduce transfer learning setups\n","\n","-  access pretrained ConvNets with a unique interface/API inspired by torchvision."]},{"cell_type":"code","metadata":{"id":"j_umLuXpmYwk"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install pretrainedmodels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLbB2aTdigvz"},"source":["# 2. Dataset Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WDHFkRyDjxpP"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"wJ0jmI1xxQ7j"},"source":["text_data_path = '/content/Rakuten/data'\n","image_data_path = '/content/Rakuten/image' "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QzA9onxAnhv"},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        # #loading the Merged, preprocessed text data and test data\n","        # train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # # new_train =  train[train['Description'] != \" \"]\n","        # # new_train = new_train[new_train['Description'].notna()]\n","        # self.train = train\n","\n","        catalog_eng= pd.read_csv(text_data_path+\"/catalog_english_taxonomy.tsv\",sep=\"\\t\")\n","        X_train= pd.read_csv(text_data_path+\"/X_train.tsv\",sep=\"\\t\")\n","        Y_train= pd.read_csv(text_data_path+\"/Y_train.tsv\",sep=\"\\t\")\n","        \n","        self.list_tags = list(Y_train['Prdtypecode'].unique())\n","        for i,tag in enumerate(self.list_tags):\n","            self.dict_code_to_id[tag] = i \n","            self.dict_id_to_code[i]=tag\n","        print(self.dict_code_to_id)\n","            \n","        Y_train['labels']=Y_train['Prdtypecode'].map(self.dict_code_to_id)\n","        train=pd.merge(left=X_train,right=Y_train,\n","               how='left',left_on=['Integer_id','Image_id','Product_id'],\n","               right_on=['Integer_id','Image_id','Product_id'])\n","        prod_map=pd.Series(catalog_eng['Top level category'].values,\n","                           index=catalog_eng['Prdtypecode']).to_dict()\n","\n","        train['product'] = train['Prdtypecode'].map(prod_map)\n","        train['title_len']=train['Title'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['desc_len']=train['Description'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['title_desc_len']=train['title_len'] + train['desc_len']\n","        train.loc[train['Description'].isnull(), 'Description'] = \" \"\n","        train['title_desc'] = train['Title'] + \" \" + train['Description']\n","        \n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  def prepare_test(self, text_col):\n","    \n","        X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","        self.X_test = X_test\n","        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","        self.test_sentences = X_test[text_col].values\n","        return self.test_sentences\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8triqwglkYZO"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"id":"Q_BYtGYwkSPk"},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'title_desc'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","# print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","# print(\"Labels: \", len(Preprocess.labels))\n","\n","# X_test = Preprocess.prepare_test(text_col)\n","# print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1MDFu47PX6-"},"source":["from bs4 import BeautifulSoup\n","Preprocess.train['Description'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Description'] ]\n","Preprocess.train['Title'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Title'] ]\n","Preprocess.train['title_desc'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['title_desc'] ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9jF_ASbMB2A"},"source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","\n","counter = Counter(Preprocess.labels)\n","for k,v in counter.items():\n","\tper = v / len(Preprocess.labels) * 100\n","\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","\n","print(\"\")\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qguSL6pMHQve"},"source":["print(Preprocess.train['Title'].isnull().sum())\n","\n","print(Preprocess.train['Description'].isnull().sum())\n","\n","print(Preprocess.train['Image_id'].isnull().sum())\n","\n","print(Preprocess.train['Product_id'].isnull().sum())\n","\n","print(Preprocess.train['Prdtypecode'].isnull().sum())\n","\n","print(Preprocess.train['labels'].isnull().sum())\n","\n","print(Preprocess.train['product'].isnull().sum()) #top level category\n","\n","print(Preprocess.train['title_desc'].isnull().sum())\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gGj4NDbINdl"},"source":["Preprocess.train.isnull().values.any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8797SZsJIVnP"},"source":["Preprocess.train.isnull().sum().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBCwmgodH5wL"},"source":["Preprocess.train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAqoRpq9zMS4"},"source":["# View Tokenizer Input "]},{"cell_type":"code","metadata":{"id":"lhxfSqOKlR94"},"source":["Preprocess.get_sentences(text_col,True)\n","sentences = Preprocess.sentences\n","\n","labels = Preprocess.labels\n","print(sentences)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXlPVw5U1pos"},"source":["print (type(sentences))\n","print()\n","# print(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xwZDh0jf9FRc"},"source":["# View Test Sentences"]},{"cell_type":"code","metadata":{"id":"Sjmz7l6rnefT"},"source":["# batch_size = 32  \n","\n","# test_sentences = Preprocess.test_sentences\n","\n","# X_test_phase1  = Preprocess.X_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QV2kcCt9Oz6"},"source":["# print(len(test_sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVemBMlCk4xW"},"source":["**Helper Function**"]},{"cell_type":"code","metadata":{"id":"WV-Xbdjxk3Ii"},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbKzQwxlrZm"},"source":["# 3. Tokenization & Input Formatting\n"," Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"Ud5qFfdmlwsk"},"source":["# 3.1. BERT Tokenizer\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"id":"AbEg-aB2l61a"},"source":["from transformers import XLMForSequenceClassification\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","from torch.nn import Dropout,Conv1d, Linear\n","from transformers.modeling_utils import SequenceSummary\n","\n","#from transformers.modeling_roberta import RobertaClassificationHead\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l3oZ1Tims3q"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","Add special tokens to the start and end of each sentence.\n","Pad & truncate all sentences to a single constant length.\n","Explicitly differentiate real tokens from padding tokens with the â€œattention maskâ€."]},{"cell_type":"markdown","metadata":{"id":"fZvQKKcxmu4L"},"source":["# 3.3. Tokenize Dataset\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"lSBvH82Amx_3"},"source":["def prep_input(sentences,labels, max_len,tokenizer):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                           # Sentence to encode.\n","                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])       # IDs of the the vocabularies in the Model's dictionary\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors. \n","    input_ids = torch.cat(input_ids, dim=0)             # Concatenates the given sequence of seq tensors in the given dimension. \n","                                                        # All tensors must  have the same shape \n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids, attention_masks, labels\n","    else:\n","        return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKdFMMmP8aT"},"source":["# 3.4. Importing Tokenizers and Input Preparation\n","\n","- Now it is time to import both Camembert and FlauBERT tokenizers from  pretained package and prepare the input using them. \n","\n","- Calling prep_input() for each model will result in the corresponding:\n","     \n","\n","1.   **input ids**\n","2.   **attention maks**\n","3.   **labels**\n","\n"," "]},{"cell_type":"code","metadata":{"id":"-BYzJChjw5AG"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAvaWiVEwq54"},"source":["input_ids_cam, attention_masks_cam, labels_cam = prep_input (sentences, labels, max_len, tokenizer_cam)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-4ArTutx-Cx"},"source":["input_ids_flau, attention_masks_flau, labels_flau  = prep_input(sentences,labels, max_len,tokenizer_flau)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XrGAY_zRX--"},"source":["# 3.5. Training & Validation Split\n","Divide up our training randomly select **10%** as a validation set off of the training set.\n","\n","While splitting, we used the following parameters:\n","\n","\n","1.   **stratify**: \n","in this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n","2.   **random_state**: \n","simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time."]},{"cell_type":"code","metadata":{"id":"nZQ0mB1NyeWa"},"source":["# val_size = 0.15\n","#basic ---------------\n","\n","# tr_inputs_cam, val_inputs_cam, _,_ = train_test_split (input_ids_cam, labels_cam, stratify = labels_cam,    \n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_cam, val_masks_cam, _,_ =   train_test_split (attention_masks_cam, labels, stratify = labels,        # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)\n","\n","\n","# tr_inputs_flau, val_inputs_flau, _,_ = train_test_split (input_ids_flau, labels_flau, stratify=labels,\n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_flau, val_masks_flau, _,_   = train_test_split (attention_masks_flau, labels,stratify=labels_flau,  # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9_fzCmV2JBI"},"source":["# tr_inputs, test_inputs_cam, tr_labels, test_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_cam, val_inputs_cam, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_cam, val_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )\n","\n","\n","\n","\n","# tr_inputs, test_inputs_flau, tr_labels, test_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_flau, val_inputs_flau, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_flau, val_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nn6cpPDt2_fW"},"source":["tr_inputs, val_inputs_cam, tr_labels, val_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_cam, test_inputs_cam, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_cam, test_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )\n","\n","\n","\n","\n","tr_inputs, val_inputs_flau, tr_labels, val_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_flau, test_inputs_flau, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_flau, test_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiE2yDIzuRO"},"source":["torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","torch.save(test_inputs_cam, \"test_inputs_cam.pt\")\n","torch.save(test_masks_cam, \"test_masks_cam.pt\")\n","\n","\n","torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","torch.save(val_masks_flau, \"val_masks_flau.pt\")\n","torch.save(test_inputs_flau, \"test_inputs_flau.pt\")\n","torch.save(test_masks_flau, \"test_masks_flau.pt\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdkITHXh0Ahs"},"source":["text_input='./'\n","\n","tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","input_ids_test_cam = torch.load(text_input + \"test_inputs_cam.pt\") \n","attention_masks_test_cam = torch.load(text_input + \"test_masks_cam.pt\") \n","\n","tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")\n","input_ids_test_flau = torch.load(text_input + \"test_inputs_flau.pt\")\n","attention_masks_test_flau = torch.load(text_input + \"test_masks_flau.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1O4EyDzCpsS"},"source":["# 4. Defining Models to be Fused\n","\n","Now, as our  data has been preprocessed, cleaned and text data was tokenzied , it is ready to be fed to the models. \n","- As a first step,  first we need to define and configure the models. \n"]},{"cell_type":"markdown","metadata":{"id":"rlj0Dz1FW7cM"},"source":["# 4.1. RESNet Model for Image Processing. \n","\n","In PyTorch, you always need to define a forward method for your neural network model. But you never have to call it explicitly.\n","Here we are defining our image processing class is subclass of nn.Module and is inheriting all methods. In the super class, nn.Module, there is a __call__ method which obtains the forward function from the subclass and calls it."]},{"cell_type":"markdown","metadata":{"id":"rnnJlj2Y3iii"},"source":["# 4.1.1.  Image Processing Model - RESNet50\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QWvMo_nV1XCb"},"source":["from torch.nn import functional as F\n","import torch.nn as nn\n","import pretrainedmodels\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained='imagenet'):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                self.base_model.load_state_dict(torch.load(resnet_model_path))\n","                )\n","            \n","        self.l0 = nn.Linear(2048, 27)  # Applies a linear transformation to the incoming data\n","        # batch_size = 2048\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","\n","        # During the training you will get batches of images, \n","        # so your shape in the forward method will get an additional batch dimension at dim0: \n","        # [batch_size, channels, height, width].\n","        \n","        x = self.base_model.features(image) \n","\n","        #extracting feature vector from network after feature leaning \n","        #This is the flatten vector \n","\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1) \n","        #adaptive_avg_pool2d : Kernel size = (input_size+target_size-1) // target_size rounded up\n","        #Then the positions of where to apply the kernel are computed as rounded equidistant points between 0 and input_size - kernel_size\n","        \n","        out = self.l0(x)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaAYA3QY1cbO"},"source":["class Identity(nn.Module):\n","  \n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr8ciZGg1Ee4"},"source":["# 4.1.2. Instaniating the Image Processing Network \n"," Now we create an instance from the SEResnext50_32x4d class that we defined and load the weights from a pretrained model, since the training is done previously. "]},{"cell_type":"code","metadata":{"id":"ZGGor-mS1zJe"},"source":["#data_path = '/content/drive/My Drive/Rakuten/'\n","\n","img_model = SEResnext50_32x4d(pretrained=None)\n","# img_model.load_state_dict(torch.load(os.path.join(data_path, 'models/RESNET_best_model.pt')))\n","\n","# img_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9gfXO9k21-o"},"source":["# 4.1.3. Prinitng Model's Params"]},{"cell_type":"code","metadata":{"id":"WOiwcopd8m8B"},"source":["img_model.l0 = Identity()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jb7I6qBq2c0E"},"source":["for param in img_model.parameters():\n","     print(type(param), param.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9iqaXH5A3Esl"},"source":["# 4.1.4. Model's Params Require No Grads\n","\n","These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","As our model is already trained and weights are assigned, then there is no need to calculate the gradients so no need to send them to the optimizer."]},{"cell_type":"code","metadata":{"id":"c0XJcLgW8u09"},"source":["# params are iterators which contain model's parameters. Usually passed to the optimizer\n","for params in img_model.parameters():\n","      params.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXULFfzkdB8"},"source":["img_model.out_proj = Identity()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWgGQqbU4nNR"},"source":["# Image Data Preparation"]},{"cell_type":"code","metadata":{"id":"4yNKxOJ2-QrA"},"source":["# # Data path\n","# text_data_path = os.path.join('/content/drive/My Drive/Rakuten')\n","# image_data_path = os.path.join('')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM6bWAoKTsx7"},"source":["def get_img_path(img_id, prd_id, path):\n","    \n","    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n","    return path + pattern"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYomnrt7wKWw"},"source":["# print(len(Preprocess.train), len(train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzgyQu0aY17J"},"source":["# Obtaining & Splitting Images "]},{"cell_type":"code","metadata":{"id":"emmAR45klxoi"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#  train 65%\n","#  validation 15%\n","#  test 20%\n","# (original training) =>  train, test => 80, 20  val_size = 0.2\n","# (train) => train, val =>  85, 15   val_size = 0.15 \n","\n","\n","train_img = train[['Image_id','Product_id','labels','product']]\n","\n","train_img['image_path'] = Preprocess.train.progress_apply(lambda x: get_img_path(x['Image_id'], x['Product_id'],\n","                                                      path = os.path.join(image_data_path, 'image_training/')),axis=1)\n","\n","\n","\n","\n","tr_df, val_df, tr_labels, val_labels = train_test_split(train_img, train_img['labels'], \n","                                           random_state=2020,\n","                                           test_size = 0.2,\n","                                           stratify=train_img['labels'])\n","\n","\n","train_df, test_df, train_labels, test_labels = train_test_split(tr_df, tr_labels, \n","                                           random_state=2020,\n","                                           test_size = 0.15,\n","                                           stratify=tr_labels)\n","\n","\n","\n","print(\"Train: \", len(train_df))\n","print(\"Val:   \", len(val_df))\n","print(\"Test:  \", len(test_df))\n","print (\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_h5_e_Gw-M9"},"source":["print(\"Original Images Df:   \",  len(train_img))\n","print(\"Train Images DF:      \" , len(train_df))\n","print(\"Validation Images DF: \",  len(val_df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIAzQAsCTzBs"},"source":["print (train_img['image_path'][0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRigVJwVBOcz"},"source":["# Image Data Augmentation\n","\n","We're going to be making use of Pytorch's transforms for preparing the input images to be used by our model. \n","\n","\n","\n","\n","\n","*   We'll need to make sure the images in the training set and validation set are the same size, so we'll be using transforms.Resize\n","*   We'll also be doing a little data augmentation, trying to improve the performance of our model by forcing it to learn about images at different angles and crops, so we'll randomly crop and rotate the images.\n","\n","*    we'll make tensors out of the images, as PyTorch works with tensors. \n","*   Finally, we'll normalize the images, which helps the network work with values that may be have a wide range of different values.\n","\n","\n","*   We then compose all our chosen transforms.\n","\n","It worth mentioning that validation transforms don't have any of the flipping or rotating, as they aren't part of our training set, so the network isn't learning about them\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-Togfn7XmpW5"},"source":["input_size = 224 # for Resnt\n","\n","# Applying Transforms to the Data\n","\n","from torchvision import datasets, models, transforms\n","\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nB9-K4lUNoqT"},"source":["# Text Processing Models - BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"markdown","metadata":{"id":"I_vkDiuRC70z"},"source":["# 4.2 CamemBERT Model"]},{"cell_type":"code","metadata":{"id":"imOtgakyCtFe"},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","  \n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            head_mask = head_mask,\n","            inputs_embeds=inputs_embeds,\n","#           output_attentions=output_attentions,\n","#           output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = outputs[0] #(B,256,768)\n","\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","\n","#       x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","\n","        x = self.dense(x)  # 768 -> 768\n","\n","        feat= torch.tanh(x) \n","\n","        logits = self.out_proj(feat) # 768 -> 27\n","\n","        outputs = (logits,) + outputs[2:] #3rd element onwards\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KstmQqyd04x"},"source":["camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_description.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtPoFtaqDAj6"},"source":["# FlauBERT Model"]},{"cell_type":"code","metadata":{"id":"a16smoYhDCmn"},"source":["num_classes = 27\n","\n","class vec_output_FlaubertForSequenceClassification(FlaubertModel):\n","    \n","    config_class = FlaubertConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = FlaubertModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.init_weights()\n","        self.dropout =  torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        langs=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        lengths=None,\n","        cache=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        \n","        \n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            langs = langs,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            lengths = lengths,\n","            cache = cache,\n","            head_mask = head_mask,\n","            inputs_embeds = inputs_embeds,\n","        )\n","\n","        #output = self.dropout(output)\n","        output = transformer_outputs[0] \n","        vec = output[:,0]\n","        \n","        \n","        #logits\n","        dense = self.dropout(vec)\n","        \n","        #classifier\n","        logits = self.classifier(dense)\n","        \n","        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n","       \n","        \n","        return outputs,dense"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2krGO8BnVfd"},"source":["# Dataset Fusion"]},{"cell_type":"code","metadata":{"id":"qIIQ5-g3gU85"},"source":["# TODO DELELTE IMAGES WITH NO DESCRIPTION\n","# From the preprocesssed file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRq24YrsnU9X"},"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","class FusionDataset(Dataset):\n","    \n","    def __init__(self, df, inputs_cam, masks_cam, inputs_flau, masks_flau, transform=None, mode='train'):\n","        self.df = df\n","        self.transform   = transform\n","        self.mode = mode\n","\n","        self.inputs_cam  = inputs_cam\n","        self.masks_cam   = masks_cam\n","\n","        self.inputs_flau  = inputs_flau\n","        self.masks_flau   = masks_flau\n","         \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        \n","        im_path = self.df.iloc[idx]['image_path']\n","        img= plt.imread(im_path)\n","        #img = cv2.imread(im_path)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        img              = img.cuda()\n","        input_id_cam     = self.inputs_cam[idx].cuda()\n","        input_mask_cam   = self.masks_cam[idx].cuda()\n","        input_id_flau    = self.inputs_flau[idx].cuda()\n","        input_mask_flau  = self.masks_flau[idx].cuda()\n","        \n","        if self.mode =='test':\n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","            \n","        else:\n","            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5RRyCzd4XSd"},"source":["a1 = torch.randn(3,10,10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_51PvBY4Zpr"},"source":["reduce_dim = nn.Conv1d(in_channels = 10 , out_channels = 1 , kernel_size= 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wryxtip-4b0m"},"source":["reduce_dim(a1).view(3,10).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uvw1I4sNSJ9_"},"source":["# Test Sentences Tokenization"]},{"cell_type":"code","metadata":{"id":"hTUa-CbEnluo"},"source":["print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","\n","# input_ids_test_flau,attention_masks_test_flau = prep_input(test_sentences, labels=None, max_len=max_len,tokenizer = tokenizer_flau)\n","\n","# input_ids_test_cam,attention_masks_test_cam = prep_input(test_sentences , labels=None, max_len=max_len,tokenizer = tokenizer_cam)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8gijJIM5pQX"},"source":["# print(type(Preprocess.test_sentences))\n","# print(len(Preprocess.test_sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXbLhSkyCilV"},"source":["# # Moodels path \n","# resnet_model_path = '/content/Rakuten/models/RESNET_best_model.pt'\n","# camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_title_description.pt' ###### TODO Change with the updated model!!!\n","# flaubert_model_path = '/content/Rakuten/models/FlauBERT_best_model_title_description.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzlptrSui4af"},"source":["# Moodels path \n","resnet_model_path = '/content/Rakuten/models/Final_RESNet_model.pt'\n","\n","camembert_model_path_title = '/content/Rakuten/models/CamemBERT_best_model_split_title.pt'\n","camembert_model_path_desc = '/content/Rakuten/models/CamemBERT_best_model_split_description.pt'\n","\n","flaubert_model_path_title = '/content/Rakuten/models/FlauBERT_best_model_split_title.pt'\n","flaubert_model_path_desc = '/content/Rakuten/models/FlauBERT_best_model_split_description.pt'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09NiYnUxfi94"},"source":["# Fuse\n"," When using pretrained models, PyTorch sets the model to be unfrozen (will have its weights adjusted) by default"]},{"cell_type":"code","metadata":{"id":"TxW8Ups_nr8O"},"source":["class vector_fusion(nn.Module):    \n","    def __init__(self):\n","        super(vector_fusion, self).__init__()\n","\n","        self.img_model = SEResnext50_32x4d(pretrained=None)\n","        self.img_model.load_state_dict(torch.load(resnet_model_path))\n","        self.img_model.l0=Identity()\n","        for params in self.img_model.parameters():\n","            params.requires_grad=False\n","\n","# ------ CamamBERT ------\n","\n","        self.cam_model_title = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_title)\n","        self.cam_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_title.out_proj = Identity()\n","\n","\n","        self.cam_model_desc = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_desc)\n","        self.cam_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_desc.out_proj = Identity()\n","\n"," # ----  FlauBERT ----- \n","\n","        \n","        self.flau_model_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_title)\n","\n","        self.flau_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_title.classifier=Identity()\n","\n","\n","      \n","        self.flau_model_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_desc)\n","\n","        self.flau_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_desc.classifier=Identity()\n","\n","\n","# ------------------------------------------------------------\n","\n","        self.reduce_dim = nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n","        self.reduce_dim2 = nn.Conv1d(in_channels = 768 , out_channels = 1 , kernel_size= 1)\n","        self.out = nn.Linear(768*3, 27)\n","        \n","        #gamma\n","#         self.w1 = nn.Parameter(torch.zeros(1))\n","#         self.w2 = nn.Parameter(torch.zeros(1))\n","#         self.w3 = nn.Parameter(torch.zeros(1))\n","        \n","    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n","        \n","        cam_emb_title,vec1_title = self.cam_model_title(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","        cam_emb_desc,vec1_desc = self.cam_model_desc(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","\n","#---------------------------------\n","        \n","        flau_emb_title,vec2 =self.flau_model_title(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","        flau_emb_desc,vec2_desc =self.flau_model_desc(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","# ---------------------------------\n","        \n","        #Projecting the image embedding to lower dimension\n","        img_emb = self.img_model(img)\n","        \n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1],1)\n","        img_emb = self.reduce_dim(img_emb)\n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n","# --------------------------------\n","\n","        #summing up the vectors\n","        cam_emb  = cam_emb_title + cam_emb_desc\n","        flau_emb = flau_emb_title + flau_emb_desc\n","        \n","        #Bilinear\n","        #text_emb = text_emb.view(text_emb.shape[0],1,text_emb.shape[1])  ##### bs * 1 * 768\n","        \n","        #Bilinear Pooling\n","        #pool_emb = torch.bmm(img_emb,text_emb) ### bs * 768 * 768\n","        #pool_emb = self.reduce_dim2(pool_emb).view(text_emb.shape[0],768)  #### bs * 1 * 768\n","        fuse= torch.cat([img_emb,cam_emb[0],flau_emb[0]],axis=1)\n","        \n","        \n","        logits = self.out(fuse)\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1VU1vrSQFzD"},"source":[" img_model = SEResnext50_32x4d(pretrained=None)\n"," img_model.load_state_dict(torch.load(resnet_model_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4LM7TANsMqJ"},"source":["cam_title= vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","checkpoint = torch.load(camembert_model_path_title)\n","cam_title.load_state_dict(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0iQCIXYsTlf"},"source":["# cam_desc= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_desc)\n","# cam_desc.load_state_dict(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZfncq4PO_EY"},"source":["# flau_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_title)\n","# flau_title.load_state_dict(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo090Yeusw5V"},"source":["# flau_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_desc)\n","# flau_desc.load_state_dict(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eU5pHNZKNKr3"},"source":["#  Instantiation  & Training of Fusion Model "]},{"cell_type":"code","metadata":{"id":"8Qpj2jQYu0P-"},"source":["model = vector_fusion() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GN9HIANRu1_R"},"source":["model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbah-djKPyRB"},"source":["# Fuse Input Data"]},{"cell_type":"code","metadata":{"id":"ZZBQFQnFSn6L"},"source":["train_dataset = FusionDataset (train_df, tr_inputs_cam, tr_masks_cam, tr_inputs_flau, tr_masks_flau,\n","                            transform = image_transforms['train'])\n","\n","val_dataset = FusionDataset (val_df, val_inputs_cam, val_masks_cam, val_inputs_flau, val_masks_flau,\n","                          transform = image_transforms['valid'])\n","\n","\n","\n","test_dataset = FusionDataset (test_df, input_ids_test_cam, attention_masks_test_cam, input_ids_test_flau, attention_masks_test_flau\n","                           , transform = image_transforms['test'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uh2PccgCXJeI"},"source":["# Data Loaders\n","\n","We need to use the DataLoaders to create iterable objects for us to work with. We tell it which datasets we want to use, give it a batch size, and shuffle the data"]},{"cell_type":"code","metadata":{"id":"nfFhYJYpSr05"},"source":["batch_size = 32  #increase batch size to reduce the noise \n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","validation_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n"," \n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2seYdimqCP-"},"source":["#Wj= n_samples/n_classes*n_samplesj\n","\n","w1 = 10190/27* 374\n","\n","w2 = 10190/27*571\n","\n","w3 = 10190/27*202\n","\n","w4 = 10190/27*584\n","\n","w5 = 10190/27*331\n","\n","w6 = 10190/27*599\n","\n","w7 = 10190/27*311\n","\n","w8 = 10190/27*609\n","\n","w9 = 10190/27*248\n","\n","w10 = 10190/27*516\n","\n","w11 = 10190/27*573\n","\n","w12 = 10190/27*321\n","\n","w13 = 10190/27*1225\n","\n","w14 = 10190/27*92\n","\n","w15 = 10190/27*605\n","\n","w16 = 10190/27*170\n","\n","w17 = 10190/27*474\n","\n","w18 = 10190/27*599\n","\n","w19 = 10190/27*301\n","\n","w20 = 10190/27*100\n","\n","w21 = 10190/27*389\n","\n","w22 = 10190/27*299\n","\n","w23 = 10190/27*99\n","\n","w24 = 10190/27*105\n","\n","w25 = 10190/27*300\n","\n","w26 = 10190/27*96\n","\n","w27 = 10190/27*97\n","\n","class_weights = torch.FloatTensor([w1, w2, w3, w4, w5, w6, w7, w8, w9, w10,\n","                                 w11, w12, w13, w14, w15, w16, w17, w18,\n","                                 w19, w20, w21, w22, w23, w24, w25, w26, w27]).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"en6CE-_8S-Vw"},"source":["import torch.nn as nn\n","loss_criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","#loss_criterion = CrossEntropyLossFlat(weight=class_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLg8d2UmTBLp"},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_lgPHpW0kv3"},"source":["# test_loader = validation_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te03NzKu28Fs"},"source":["ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMNZStqu0ZzH"},"source":["# cp './Hirarical_add_concat_best_model.pt' '../drive/My Drive/Rakuten/models/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkB48fg1HidV"},"source":["# Model Testing"]},{"cell_type":"code","metadata":{"id":"XjYObOyZ-GAd"},"source":["model_path =  '/content/drive/My Drive/Rakuten/models/Final_Hirarical_model.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKTbBLHzHoYL"},"source":["checkpoint = torch.load(model_path)\n","model.load_state_dict(checkpoint) # A state_dict is simply a Python dictionary object \n","                                  # that maps each layer to its parameter tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46UaiCmcHwR4"},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","\n","\n","def predict_pyt(model, prediction_dataloader):\n","    \"\"\"\n","    model: pytorch model\n","    prediction_dataloader: DataLoader object for which the predictions has to be made.\n","    return:\n","        predictions:    - Direct predicted labels\n","        softmax_logits: - logits which are normalized with softmax on output\"\"\"\n","\n","    pred = []\n","    ground_truth = []\n","    \n","    print(\"\")\n","    print(\"Running Testing...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    logits_values =[]\n","    val_accuracy_values = []\n","    val_loss_values = []\n","    \n","    \n","    total_t0 = time.time()\n","    # Evaluate data for one epoch\n","    for batch in (prediction_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","\n","\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(prediction_dataloader)\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(prediction_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Test took: {:}\".format(validation_time))\n","    print(\"Test F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1 = f1_score(true_labels,predictions,average='macro')\n","\n","    # pred.append(predictions)\n","    # ground_truth.append(true_labels) \n","\n","    print(\"\")\n","    print(\"Test complete!\")\n","\n","    # Results = pd.DataFrame(\n","    # {'Pred':    predictions,\n","    #  'Ground': true_labels\n","    # })\n","\n","    # #print(\"Total testing took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    # print()\n","    # plt.plot(np.array(val_accuracy_values), 'r', label='Test accuracy')\n","    # plt.legend()\n","    # plt.title('Test Curve')\n","    # plt.show()\n","\n","    \n","    # print(Results)\n","\n","    # Results.to_csv( \"Baseline_80_20\")\n","\n","    print()\n","    print('DONE')\n","\n","    return predictions, true_labels #, softmax_logits\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwK3RdRpLy30"},"source":["predictions_val, true_label = predict_pyt(model, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFNyUjdU4HsO"},"source":[" Results = pd.DataFrame(\n","    {'Pred':    predictions_val,\n","     'Ground': true_label\n","    })\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXH_emHM4MX2"},"source":["Results.to_csv( \"Hirachical_80_20\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1SpIUuc4P-0"},"source":["Results = pd.read_csv('Hirachical_80_20')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAwZeQmU4ZrL"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(Results['Ground'], Results['Pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-DcuRhp4SCC"},"source":["print(f1_score(Results['Ground'],Results['Pred'],average='macro'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3wDct5D4Tw_"},"source":["from sklearn.metrics import confusion_matrix\n","confusion_mat = confusion_matrix(Results['Ground'], Results['Pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQIbNJHY4Wrk"},"source":["import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df_cm = pd.DataFrame(confusion_mat, index = [i for i in range(27)],\n","                                    columns = [i for i in range(27)])\n","plt.figure(figsize = (30,30))\n","sn.heatmap(df_cm, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcfs62Fb4c3X"},"source":["from sklearn.metrics import classification_report\n","\n","target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6',\n","                'class 7', 'class 8', 'class 9', 'class 10', 'class 11', 'class 12', \n","                'class 13', 'class 14', 'class 15', 'class 16', 'class 17', 'class 18',\n","                'class 19','class 20', 'class 21', 'class 22', 'class 23', 'class 24', 'class 25', 'class 26', 'class 27']\n","\n","print(classification_report(Results['Ground'], Results['Pred'], target_names=target_names))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOg18jE90FAN"},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","n_classes = 27\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","# for i in range(n_classes):  \n","    # fpr[i], tpr[i], _ = roc_curve(true_label[:,i], predictions_val[:, i])\n","    # roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","fpr, tpr, _ = roc_curve(true_label, predictions_val)\n","roc_auc = auc(fpr, tpr)\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","# Plot of a ROC curve for a specific class\n","plt.figure()\n","plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic example')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","         label='micro-average ROC curve (area = {0:0.2f})'\n","               ''.format(roc_auc[\"micro\"]))\n","\n","plt.plot(fpr, tpr, label='ROC curve of class {0} (area = {1:0.2f})'\n","                                   ''.format(i, roc_auc))\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Some extension of Receiver operating characteristic to multi-class')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdC6g5FC0yjH"},"source":["print(true_label.type)\n","print(predictions_val.type)"],"execution_count":null,"outputs":[]}]}